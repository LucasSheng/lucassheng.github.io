@article{wang_vpu_2022,
 abstract = {In this work, we propose a new patch-based framework called VPU for the video-based point cloud upsampling task by effectively exploiting temporal dependency among multiple consecutive point cloud frames, in which each frame consists of a set of unordered, sparse and irregular 3D points. Rather than adopting the sophisticated motion estimation strategy in video analysis, we propose a new spatio-temporal aggregation (STA) module to effectively extract, align and aggregate rich local geometric clues from consecutive frames at the feature level. By more reliably summarizing spatio-temporally consistent and complementary knowledge from multiple frames in the resultant local structural features, our method better infers the local geometry distributions at the current frame. In addition, our STA module can be readily incorporated with various existing single frame-based point upsampling methods (e.g., PU-Net, MPU, PU-GAN and PU-GCN). Comprehensive experiments on multiple point cloud sequence datasets demonstrate our video-based point cloud upsampling framework achieves substantial performance improvement over its single frame-based counterparts.},
 author = {Wang, Kaisiyuan and Sheng, Lu and Gu, Shuhang and Xu, Dong},
 doi = {10.1109/TIP.2022.3166627},
 file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/GA9Q9MUD/9759233.html:text/html},
 issn = {1941-0042},
 journal = {IEEE Transactions on Image Processing},
 keywords = {Cloud computing, Feature extraction, Graphics processing units, Image reconstruction, Point cloud compression, Point cloud sequence, point cloud upsampling, spatial-temporal aggregation, Task analysis, Three-dimensional displays},
 note = {Conference Name: IEEE Transactions on Image Processing},
 pages = {4062--4075},
 shorttitle = {VPU},
 title = {VPU: A Video-Based Point Cloud Upsampling Framework},
 volume = {31},
 year = {2022}
}

