<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.6.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Lu Sheng" />

  
  
  
    
  
  <meta name="description" content="Associate Professor" />

  
  <link rel="alternate" hreflang="en-us" href="https://lucassheng.github.io/publication/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.e16b071c3437dd38fe4bc6049c3c2fff.css" />

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



  


  


  




  
  
  

  
  
    <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="Lu Sheng (盛律)" />
  

  
  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://lucassheng.github.io/publication/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Lu Sheng (盛律)" />
  <meta property="og:url" content="https://lucassheng.github.io/publication/" />
  <meta property="og:title" content="Publications | Lu Sheng (盛律)" />
  <meta property="og:description" content="Associate Professor" /><meta property="og:image" content="https://lucassheng.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://lucassheng.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2022-10-23T00:00:00&#43;00:00" />
    
  

  



  


  <title>Publications | Lu Sheng (盛律)</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3a079e7dad19be978a318345a7749d34" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js"></script>

  




  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Lu Sheng (盛律)</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Lu Sheng (盛律)</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#news"><span>News</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    















  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search form-control form-control-sm" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-2">
              Journal article
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2021">
              2021
            </option>
            
            <option value=".year-2020">
              2020
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            <option value=".year-2018">
              2018
            </option>
            
            <option value=".year-2017">
              2017
            </option>
            
            <option value=".year-2016">
              2016
            </option>
            
            <option value=".year-2015">
              2015
            </option>
            
            <option value=".year-2014">
              2014
            </option>
            
            <option value=".year-2013">
              2013
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Gengshi Huang</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Jianing Teng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Wang Kun</span>, <span >
      Zhenfei Yin</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Ziwei Liu</span>, <span >
      Yu Qiao</span>, <span >
      Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>
  </span>
  (2022).
  <a href="/publication/he-x-learner-2022/">X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation</a>.
  European Conference on Computer Vision (<strong>ECCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2203.08764" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/he-x-learner-2022/cite.bib">
  Cite
</a>














</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Chenjian Gao</span>, <span >
      Qian Yu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Yi-Zhe Song</span>, <span >
      Dong Xu</span>
  </span>
  (2022).
  <a href="/publication/gao-sketchsampler-2022/">SketchSampler: Sketch-based 3D Reconstruction via View-dependent Depth Sampling</a>.
  European Conference on Computer Vision (<strong>ECCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2208.06880v1.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/gao-sketchsampler-2022/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  










</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Ziming Wang</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Xiaoliang Huo</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Zhenghao Chen</span>, <span >
      Jing Zhang</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Dong Xu</span>
  </span>
  (2022).
  <a href="/publication/wang-improving-2022/">Improving RGB-D Point Cloud Registration by Learning Multi-scale Local Linear Transformation</a>.
  European Conference on Computer Vision (<strong>ECCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2208.14893" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/wang-improving-2022/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/514dna/llt" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/514dna/llt" target="_blank" rel="noopener">
  Project
</a>










</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Daigang Cai</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Lichen Zhao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribbution"></i>, <span >
      Jing Zhang</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Dong Xu</span>
  </span>
  (2022).
  <a href="/publication/cai-3-djcg-2022/">3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style="color:red"><strong>Oral Presentation</strong></span>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_3DJCG_A_Unified_Framework_for_Joint_Dense_Captioning_and_Visual_CVPR_2022_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cai-3-djcg-2022/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/zlccccc/3DJCG" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/language-for-3d-scenes/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="http://kaldir.vc.in.tum.de/scanrefer_benchmark/" target="_blank" rel="noopener">
    <i class="fab fa-hackerrank mr-1"></i>Ranks in ScanRefer Benchmark</a>

</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Buyu Li</span>, <span >
      Yongchi Zhao</span>, <span >
      Zhelun Shi</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>
  </span>
  (2022).
  <a href="/publication/li-danceformer-2022/">DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer</a>.
  AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ojs.aaai.org/index.php/AAAI/article/view/20014" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/li-danceformer-2022/cite.bib">
  Cite
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://huiye-tech.github.io/post/danceformer/" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v36i2.20014" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Kaisiyuan Wang,</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Shuhang Gu,</span>, <span >
      Dong Xu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>
  </span>
  (2022).
  <a href="/publication/wang-vpu-2022/">VPU: A Video-Based Point Cloud Upsampling Framework</a>.
  IEEE Transactions on Image Processing (<strong>IEEE T-IP</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/wang-vpu-2022/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TIP.2022.3166627" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Guanze Liu</span>, <span >
      Yu Rong</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>
  </span>
  (2021).
  <a href="/publication/liu-votehmr-2021/">VoteHMR: Occlusion-Aware Voting Network for Robust 3D Human Mesh Recovery from Partial Point Clouds</a>.
  ACM International Conference on Multimedia (<strong>ACM MM</strong>), <span style="color:red"><strong>Oral Presentation</strong></span>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2110.08729" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/liu-votehmr-2021/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/hanabi7/VoteHMR" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3474085.3475309" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Xiaolei Wu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Zhihao Hu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Dong Xu</span>
  </span>
  (2021).
  <a href="/publication/wu-styleformer-2021/">StyleFormer: Real-time Arbitrary Style Transfer via Parametric Style Composition</a>.
  IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_StyleFormer_Real-Time_Arbitrary_Style_Transfer_via_Parametric_Style_Composition_ICCV_2021_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/wu-styleformer-2021/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/Wxl-stars/PytorchStyleFormer" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV48922.2021.01435" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Lichen Zhao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Daigang Cai</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Dong Xu</span>
  </span>
  (2021).
  <a href="/publication/zhao-3-dvg-transformer-2021/">3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds</a>.
  IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/zhao-3-dvg-transformer-2021/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/zlccccc/3DVG-Transformer" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/language-for-3d-scenes/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV48922.2021.00292" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="http://kaldir.vc.in.tum.de/scanrefer_benchmark/" target="_blank" rel="noopener">
    <i class="fab fa-hackerrank mr-1"></i>Ranks in ScanRefer Benchmark</a>

</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Kaisiyuan Wang,</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Shuhang Gu,</span>, <span >
      Dong Xu</span>
  </span>
  (2021).
  <a href="/publication/wang-sequential-2021/">Sequential Point Cloud Upsampling by Exploiting Multi-Scale Temporal Dependency</a>.
  IEEE Transactions on Circuits and Systems for Video Technology (<strong>IEEE T-CSVT</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/wang-sequential-2021/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TCSVT.2021.3104304" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Lichen Zhao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Jinyang Guo,</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Dong Xu</span>, <span class="author-highlighted">
      Lu Sheng</span>
  </span>
  (2021).
  <a href="/publication/zhao-transformer-3d-det-2021/">Transformer3D-Det: Improving 3D Object Detection by Vote Refinement</a>.
  IEEE Transactions on Circuits and Systems for Video Technology (<strong>IEEE T-CSVT</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/zhao-transformer-3d-det-2021/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TCSVT.2021.3102025" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Bei Gan</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Yichun Zhou</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Guojun Yin</span>, <span >
      Luchuan Song</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Jing Shao</span>, <span >
      Ziwei Liu</span>
  </span>
  (2021).
  <a href="/publication/he-forgerynet-2021/">ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style="color:red"><strong>Oral Presentation</strong></span>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/he-forgerynet-2021/cite.bib">
  Cite
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://yinanhe.github.io/projects/forgerynet.html#download" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR46437.2021.00434" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://competitions.codalab.org/competitions/33386" target="_blank" rel="noopener">
    <i class="fa fa-trophy mr-1"></i>ForgeryNet Challenge</a>

</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Bowen Cheng</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Shaoshuai Shi</span>, <span >
      Ming Yang</span>, <span >
      Dong Xu</span>
  </span>
  (2021).
  <a href="/publication/cheng-back-tracing-2021/">Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Cheng_Back-Tracing_Representative_Points_for_Voting-Based_3D_Object_Detection_in_Point_CVPR_2021_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cheng-back-tracing-2021/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/cheng052/BRNet" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR46437.2021.00885" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://paperswithcode.com/paper/back-tracing-representative-points-for-voting" target="_blank" rel="noopener">
    <i class="fab fa-hackerrank mr-1"></i>Ranks in Papers with Code</a>

</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Rui Su</span>, <span >
      Dong Xu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Wanli Ouyang</span>
  </span>
  (2021).
  <a href="/publication/su-pcg-tal-2021/">PCG-TAL: Progressive Cross-Granularity Cooperation for Temporal Action Localization</a>.
  IEEE Transactions on Image Processing (<strong>IEEE T-IP</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/su-pcg-tal-2021/cite.bib">
  Cite
</a>





  
    
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TIP.2020.3044218" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Chi Ho Cheung</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      King Ngi Ngan</span>
  </span>
  (2021).
  <a href="/publication/cheung-motion-2021/">Motion Compensated Virtual View Synthesis Using Novel Particle Cell</a>.
  IEEE Transactions on Multimedia (<strong>IEEE T-MM</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cheung-motion-2021/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TMM.2020.3004966" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Yandan Yang</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Xiaolong Jiang</span>, <span >
      Haochen Wang</span>, <span >
      Dong Xu</span>, <span >
      Xianbin Cao</span>
  </span>
  (2021).
  <a href="/publication/yang-increaco-2021/">IncreACO: Incrementally Learned Automatic Check-out with Photorealistic Exemplar Augmentation</a>.
  IEEE Winter Conference on Applications of Computer Vision (<strong>WACV</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/yang-increaco-2021/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/WACV48630.2021.00067" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/9423423/" target="_blank" rel="noopener">
    URL</a>

</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution, Corresponding author"></i>, <span >
      Junting Pan</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Jiaming Guo</span>, <span >
      Jing Shao</span>, <span >
      Chen Change Loy</span>
  </span>
  (2020).
  <a href="/publication/sheng-high-quality-2020/">High-Quality Video Generation from Static Structural Annotations</a>.
  International Journal of Computer Vision (<strong>IJCV</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-high-quality-2020/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/junting/seg2vid." target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s11263-020-01334-x" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Yuyang Qian</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Guojun Yin</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution, Corresponding author"></i>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Zixuan Chen</span>, <span >
      Jing Shao</span>
  </span>
  (2020).
  <a href="/publication/vedaldi-thinking-2020/">Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues</a>.
  European Conference on Computer Vision (<strong>ECCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570086.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vedaldi-thinking-2020/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-030-58610-2_6" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/yyk-wew/F3Net" target="_blank" rel="noopener">
    Unofficial implementation</a>

</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Ronghao Guo</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Chen Lin</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Chuming Li</span>, <span >
      Keyu Tian</span>, <span >
      Ming Sun</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Junjie Yan</span>
  </span>
  (2020).
  <a href="/publication/vedaldi-powering-2020/">Powering One-Shot Topological NAS with Stabilized Share-Parameter Proxy</a>.
  European Conference on Computer Vision (<strong>ECCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590613.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vedaldi-powering-2020/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-030-58568-6_37" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Minghua Liu</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Sheng Yang</span>, <span >
      Jing Shao</span>, <span >
      Shi-Min Hu</span>
  </span>
  (2020).
  <a href="/publication/liu-morphing-2020/">Morphing and Sampling Network for Dense Point Cloud Completion</a>.
  AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ojs.aaai.org/index.php/AAAI/article/view/6827" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/liu-morphing-2020/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/Colin97/MSN-Point-Cloud-Completion" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/drive/folders/1X143kUwtRtoPFxNRvUk9LuPlsf1lLKI7" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v34i07.6827" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Dan Xu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Wanli Ouyang</span>, <span >
      Xiaogang Wang</span>
  </span>
  (2019).
  <a href="/publication/sheng-unsupervised-2019/">Unsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM</a>.
  IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Sheng_Unsupervised_Collaborative_Learning_of_Keyframe_Detection_and_Visual_Odometry_Towards_ICCV_2019_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-unsupervised-2019/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2019.00440" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Chufeng Tang</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Zhao-Xiang Zhang</span>, <span >
      Xiaolin Hu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>
  </span>
  (2019).
  <a href="/publication/tang-improving-2019/">Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization</a>.
  IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_Improving_Pedestrian_Attribute_Recognition_With_Weakly-Supervised_Multi-Scale_Attribute-Specific_Localization_ICCV_2019_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/tang-improving-2019/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/chufengt/ALM-pedestrian-attribute" target="_blank" rel="noopener">
  Code
</a>






  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://chufengt.github.io/publication/pedestrian-attribute/iccv_poster_id2029.pdf" target="_blank" rel="noopener">
  Poster
</a>







<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2019.00510" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Zihao Wang</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Xihui Liu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Hongsheng Li</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Junjie Yan</span>, <span >
      Xiaogang Wang</span>, <span >
      Jing Shao</span>
  </span>
  (2019).
  <a href="/publication/wang-camp-2019/">CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval</a>.
  IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CAMP_Cross-Modal_Adaptive_Message_Passing_for_Text-Image_Retrieval_ICCV_2019_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/wang-camp-2019/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/ZihaoWang-CV/CAMP_iccv19" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2019.00586" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Bowen Dong</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>
  </span>
  (2019).
  <a href="/publication/dong-bags-2019/">Bags of tricks for learning depth and camera motion from monocular videos</a>.
  Virtual Reality &amp; Intelligent Hardware (<strong>VRIH</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dong-bags-2019/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.vrih.2019.09.004" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Fanzi Wu</span>, <span >
      Songnan Li</span>, <span >
      Tianhao Zhao</span>, <span >
      King Ngi Ngan</span>, <span class="author-highlighted">
      Lu Sheng</span>
  </span>
  (2019).
  <a href="/publication/wu-cascaded-2019/">Cascaded regression using landmark displacement for 3D face reconstruction</a>.
  Pattern Recognition Letters (<strong>PRL</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/wu-cascaded-2019/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.patrec.2019.07.017" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Junting Pan</span>, <span >
      Chengyu Wang</span>, <span >
      Xu Jia</span>, <span >
      Jing Shao</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Junjie Yan</span>, <span >
      Xiaogang Wang</span>
  </span>
  (2019).
  <a href="/publication/pan-video-2019/">Video Generation From Single Semantic Label Map</a>.
  2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Pan_Video_Generation_From_Single_Semantic_Label_Map_CVPR_2019_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/pan-video-2019/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/junting/seg2vid" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2019.00385" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Guojun Yin</span>, <span >
      Bin Liu</span>, <span class="author-highlighted">
      Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Nenghai Yu</span>, <span >
      Xiaogang Wang</span>, <span >
      Jing Shao</span>
  </span>
  (2019).
  <a href="/publication/yin-semantics-2019/">Semantics Disentangling for Text-To-Image Generation</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style="color:red"><strong>Oral Presentation</strong></span>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Semantics_Disentangling_for_Text-To-Image_Generation_CVPR_2019_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/yin-semantics-2019/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/gjyin91/SDGAN" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2019.00243" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Buyu Li</span>, <span >
      Wanli Ouyang</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Xingyu Zeng</span>, <span >
      Xiaogang Wang</span>
  </span>
  (2019).
  <a href="/publication/li-gs-3-d-2019/">GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_GS3D_An_Efficient_3D_Object_Detection_Framework_for_Autonomous_Driving_CVPR_2019_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/li-gs-3-d-2019/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2019.00111" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Guojun Yin</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Bin Liu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Nenghai Yu</span>, <span >
      Xiaogang Wang</span>, <span >
      Jing Shao</span>
  </span>
  (2019).
  <a href="/publication/yin-context-2019/">Context and Attribute Grounded Dense Captioning</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Context_and_Attribute_Grounded_Dense_Captioning_CVPR_2019_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/yin-context-2019/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/gjyin91/CAG-Net" target="_blank" rel="noopener">
  Code
</a>




  
    
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2019.00640" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2019">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Jianfei Cai</span>, <span >
      Tat-Jen Cham</span>, <span >
      Vladimir Pavlovic</span>, <span >
      King Ngi Ngan</span>
  </span>
  (2019).
  <a href="/publication/sheng-visibility-2019/">Visibility Constrained Generative Model for Depth-Based 3D Facial Pose Tracking</a>.
  IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE T-PAMI</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-visibility-2019/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TPAMI.2018.2877675" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Yongcheng Liu</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Junjie Yan</span>, <span >
      Shiming Xiang</span>, <span >
      Chunhong Pan</span>
  </span>
  (2018).
  <a href="/publication/liu-multi-label-2018/">Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection</a>.
  ACM international conference on Multimedia (<strong>ACM MM</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1809.05884" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/liu-multi-label-2018/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/Yochengliu/MLIC-KD-WSD" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://yochengliu.github.io/MLIC-KD-WSD/" target="_blank" rel="noopener">
  Project
</a>









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240567" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Guojun Yin</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Bin Liu</span>, <span >
      Nenghai Yu</span>, <span >
      Xiaogang Wang</span>, <span >
      Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Chen Change Loy</span>
  </span>
  (2018).
  <a href="/publication/ferrari-zoom-net-2018/">Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition</a>.
  European Conference on Computer Vision (<strong>ECCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Guojun_Yin_Zoom-Net_Mining_Deep_ECCV_2018_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ferrari-zoom-net-2018/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/gjyin91/ZoomNet" target="_blank" rel="noopener">
  Code
</a>




  
    
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-030-01219-9_20" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2018">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Chi Ho Cheung</span>, <span >
      King Ngi Ngan</span>, <span class="author-highlighted">
      Lu Sheng</span>
  </span>
  (2018).
  <a href="/publication/cheung-spatio-temporal-2018/">Spatio-Temporal Disocclusion Filling Using Novel Sprite Cells</a>.
  IEEE Transactions on Multimedia (<strong>IEEE T-MM</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cheung-spatio-temporal-2018/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TMM.2017.2772442" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Shuyang Sun</span>, <span >
      Zhanghui Kuang</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Wanli Ouyang</span>, <span >
      Wei Zhang</span>
  </span>
  (2018).
  <a href="/publication/sun-optical-2018/">Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Optical_Flow_Guided_CVPR_2018_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sun-optical-2018/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kevin-ssy/Optical-Flow-Guided-Feature" target="_blank" rel="noopener">
  Code
</a>




  
    
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2018.00151" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Yu Liu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Fangyin Wei</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Junjie Yan</span>, <span >
      Xiaogang Wang</span>
  </span>
  (2018).
  <a href="/publication/liu-exploring-2018/">Exploring Disentangled Feature Representation Beyond Face Identification</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Exploring_Disentangled_Feature_CVPR_2018_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/liu-exploring-2018/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2018.00222" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Ziyi Lin</span>, <span >
      Jing Shao</span>, <span >
      Xiaogang Wang</span>
  </span>
  (2018).
  <a href="/publication/sheng-avatar-net-2018/">Avatar-Net: Multi-scale Zero-Shot Style Transfer by Feature Decoration</a>.
  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-avatar-net-2018/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/LucasSheng/avatar-net" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/image-and-video-synthesis/">
    Project
  </a>
  







  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=amaeqbw6TeA" target="_blank" rel="noopener">
  Video
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2018.00860" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2017">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Xihui Liu</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Haiyu Zhao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      Maoqing Tian</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Corresponding author"></i>, <span >
      Shuai Yi</span>, <span >
      Junjie Yan</span>, <span >
      Xiaogang Wang</span>
  </span>
  (2017).
  <a href="/publication/liu-hydraplus-net-2017/">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</a>.
  IEEE International Conference on Computer Vision (<strong>ICCV</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_HydraPlus-Net_Attentive_Deep_ICCV_2017_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/liu-hydraplus-net-2017/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/xh-liu/HydraPlus-Net" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/drive/folders/0B5_Ra3JsEOyOUlhKM0VPZ1ZWR2M?resourcekey=0-CdctEkdX1j2GSMSWWfrPSQ" target="_blank" rel="noopener">
  Dataset
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://xh-liu.github.io/HydraPlus-Net/" target="_blank" rel="noopener">
  Project
</a>









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2017.46" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2017">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span>, <span >
      Jianfei Cai</span>, <span >
      Tat-Jen Cham</span>, <span >
      Vladimir Pavlovic</span>, <span >
      King Ngi Ngan</span>
  </span>
  (2017).
  <a href="/publication/sheng-generative-2017/">A Generative Model for Depth-Based Robust 3D Facial Pose Tracking</a>.
  IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>).
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Sheng_A_Generative_Model_CVPR_2017_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-generative-2017/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2017.489" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2016">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Songnan Li</span>, <span >
      King Ngi Ngan</span>, <span >
      Raveendran Paramesran</span>, <span class="author-highlighted">
      Lu Sheng</span>
  </span>
  (2016).
  <a href="/publication/li-real-time-2016/">Real-Time Head Pose Tracking with Online Face Template Reconstruction</a>.
  IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE T-PAMI</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/li-real-time-2016/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TPAMI.2015.2500221" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2015">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span>, <span >
      King Ngi Ngan</span>, <span >
      Chern-Loon Lim</span>, <span >
      Songnan Li</span>
  </span>
  (2015).
  <a href="/publication/sheng-online-2015/">Online Temporally Consistent Indoor Depth Video Enhancement via Static Structure</a>.
  IEEE Transactions on Image Processing (<strong>IEEE T-IP</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-online-2015/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TIP.2015.2416658" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2015">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Chi Ho Cheung</span>, <span class="author-highlighted">
      Lu Sheng</span>, <span >
      King Ngi Ngan</span>
  </span>
  (2015).
  <a href="/publication/cheung-disocclusion-2015/">A disocclusion filling method using multiple sprites with depth for virtual view synthesis</a>.
  IEEE International Conference on Multimedia &amp; Expo Workshops (<strong>ICMEW</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cheung-disocclusion-2015/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICMEW.2015.7169773" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2015">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span>, <span >
      King Ngi Ngan</span>, <span >
      Tak-Wai Hui</span>
  </span>
  (2015).
  <a href="/publication/sheng-accelerating-2015/">Accelerating the Distribution Estimation for the Weighted Median/Mode Filters</a>.
  Asian Conference on Computer Vision (<strong>ACCV</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-accelerating-2015/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-319-16817-3_1" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2014">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span>, <span >
      King Ngi Ngan</span>, <span >
      Songnan Li</span>
  </span>
  (2014).
  <a href="/publication/sheng-temporal-2014/">Temporal depth video enhancement based on intrinsic static structure</a>.
  IEEE International Conference on Image Processing (<strong>ICIP</strong>), <span style="color:red"><strong>Oral Presentation</strong></span>.
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-temporal-2014/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICIP.2014.7025585" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2014">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Songnan Li</span>, <span >
      King Ngi Ngan</span>, <span class="author-highlighted">
      Lu Sheng</span>
  </span>
  (2014).
  <a href="/publication/li-screen-camera-2014/">Screen-camera calibration using a thread</a>.
  IEEE International Conference on Image Processing (<strong>ICIP</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/li-screen-camera-2014/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICIP.2014.7025698" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2013">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="author-highlighted">
      Lu Sheng</span>, <span >
      King Ngi Ngan</span>
  </span>
  (2013).
  <a href="/publication/sheng-depth-2013/">Depth enhancement based on hybrid geometric hole filling strategy</a>.
  IEEE International Conference on Image Processing (<strong>ICIP</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sheng-depth-2013/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICIP.2013.6738448" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2013">
          


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Songnan Li</span>, <span >
      King Ngi Ngan</span>, <span class="author-highlighted">
      Lu Sheng</span>
  </span>
  (2013).
  <a href="/publication/li-head-2013/">A Head Pose Tracking System Using RGB-D Camera</a>.
  International Conference on Computer Vision Systems (<strong>ICVS</strong>).
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/li-head-2013/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/3d-perception-and-modeling/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-642-39402-7_16" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

        </div>

        
      </div>

    </div>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  

  

  

  
  






  
  <p class="powered-by copyright-license-text">
    © 2022 Lu Sheng. Last updated Aug 30, 2022.
  </p>
  




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.46271ef31da3f018e9cd1b59300aa265.js"></script>




  
    <script src="https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js" integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
  

  
  

  






























<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type="module"></script>









<script src="/en/js/wowchemy.min.6ae253cfe39dce6eae1aa243112736e8.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>
















</body>
</html>
