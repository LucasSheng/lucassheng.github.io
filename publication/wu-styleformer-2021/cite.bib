@inproceedings{wu_styleformer_2021,
 abstract = {In this work, we propose a new feed-forward arbitrary style transfer method, referred to as StyleFormer, which can simultaneously fulfill fine-grained style diversity and semantic content coherency. Specifically, our transformerinspired feature-level stylization method consists of three modules: (a) the style bank generation module for sparse but compact parametric style pattern extraction, (b) the transformer-driven style composition module for contentguided global style composition, and (c) the parametric content modulation module for flexible but faithful stylization. The output stylized images are impressively coherent with the content structure, sensitive to the detailed style variations, but still holistically adhere to the style distributions from the style images. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our StyleFormer outperforms the existing SOTA methods in generating visually plausible stylization results with real-time efficiency.},
 address = {Montreal, QC, Canada},
 author = {Wu, Xiaolei and Hu, Zhihao and Sheng, Lu and Xu, Dong},
 booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV48922.2021.01435},
 file = {Wu et al. - 2021 - StyleFormer Real-time Arbitrary Style Transfer vi.pdf:/Users/lucasjing/Zotero/storage/AD4I8V8M/Wu et al. - 2021 - StyleFormer Real-time Arbitrary Style Transfer vi.pdf:application/pdf},
 isbn = {978-1-66542-812-5},
 language = {en},
 month = {October},
 pages = {14598--14607},
 publisher = {IEEE},
 shorttitle = {StyleFormer},
 title = {StyleFormer: Real-time Arbitrary Style Transfer via Parametric Style Composition},
 url = {https://ieeexplore.ieee.org/document/9711417/},
 urldate = {2022-08-22},
 year = {2021}
}

