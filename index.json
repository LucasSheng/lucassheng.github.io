
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Dr. Lu Sheng is an Associate Professor (since 2019) at the College of Software in Beihang University (BUAA), Beijing, China. Previously, he was a postdoctoral researcher (2017-2019) in MMLab@CUHK, with Prof. Xiaogang Wang.\nLu Sheng received his Ph.D. (2011-2016) at the Department of Electronic Engineering in the Chinese University of Hong Kong (CUHK), advised by Prof. King Ngi Ngan. He also has an internship (2015-2016) in Nanyang Technological University (NTU), with Prof. Jianfei Cai.\nHis research interests include computer vision, machine learning and multimedia, aiming at endowing machines with the capability to perceive, understand, reconstruct, and interact with the 3D visual world, with the following focuses recently:\nData-driven models for extracting hierarchical 3D semantics, inferring semantical/geometrical relationships, and rendering high-fidelity 2D/3D contents, based on multi-modal signals (including 2D/3D vision, language, etc) and beyond. Lu Sheng refers to 盛律 (How to pronounce it?) in Chinese characters, while 律 may also be rendered as Lü, Lv or Lyu in English in different circumstances.\nI am looking for talented students targeted to Master or Ph.D. degree, please drop me an email if you are interested.\n","date":1666483200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1666483200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Dr. Lu Sheng is an Associate Professor (since 2019) at the College of Software in Beihang University (BUAA), Beijing, China. Previously, he was a postdoctoral researcher (2017-2019) in MMLab@CUHK, with","tags":null,"title":"Lu Sheng","type":"authors"},{"authors":["Ziming Wang","Xiaoliang Huo","Zhenghao Chen","Jing Zhang","Lu Sheng","Dong Xu"],"categories":[],"content":"","date":1666483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666483200,"objectID":"f33b5b6c2b4c505101c70fee5f59ada9","permalink":"https://lucassheng.github.io/publication/wang-improving-2022/","publishdate":"2022-09-05T14:52:57+08:00","relpermalink":"/publication/wang-improving-2022/","section":"publication","summary":"Point cloud registration aims at estimating the geometric transformation between two point cloud scans, in which point-wise correspondence estimation is the key to its success. In addition to previous methods that seek correspondences by hand-crafted or learnt geometric features, recent point cloud registration methods have tried to apply RGB-D data to achieve more accurate correspondence. However, it is not trivial to effectively fuse the geometric and visual information from these two distinctive modalities, especially for the registration problem. In this work, we propose a new Geometry-Aware Visual Feature Extractor (GAVE) that employs multi-scale local linear transformation to progressively fuse these two modalities, where the geometric features from the depth data act as the geometry-dependent convolution kernels to transform the visual features from the RGB data. The resultant visual-geometric features are in canonical feature spaces with alleviated visual dissimilarity caused by geometric changes, by which more reliable correspondence can be achieved. The proposed GAVE module can be readily plugged into recent RGB-D point cloud registration framework. Extensive experiments on 3D Match and ScanNet demonstrate that our method outperforms the state-of-the-art point cloud registration methods even without correspondence or pose supervision. The code is available at: https://github.com/514DNA/LLT.","tags":[],"title":"Improving RGB-D Point Cloud Registration by Learning Multi-scale Local Linear Transformation","type":"publication"},{"authors":["Chenjian Gao","Qian Yu","Lu Sheng","Yi-Zhe Song","Dong Xu"],"categories":[],"content":"","date":1666483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666483200,"objectID":"c447fcddaaceeab01dbce149284f5cdf","permalink":"https://lucassheng.github.io/publication/gao-sketchsampler-2022/","publishdate":"2022-09-05T14:53:16+08:00","relpermalink":"/publication/gao-sketchsampler-2022/","section":"publication","summary":"Reconstructing a 3D shape based on a single sketch image is challenging due to the large domain gap between a sparse, irregular sketch and a regular, dense 3D shape. Existing works try to employ the global feature extracted from sketch to directly predict the 3D coordinates, but they usually suffer from losing fine details that are not faithful to the input sketch. Through analyzing the 3D-to-2D projection process, we notice that the density map that characterizes the distribution of 2D point clouds (i.e., the probability of points projected at each location of the projection plane) can be used as a proxy to facilitate the reconstruction process. To this end, we first translate a sketch via an image translation network to a more informative 2D representation that can be used to generate a density map. Next, a 3D point cloud is reconstructed via a two-stage probabilistic sampling process: first recovering the 2D points (i.e., the x and y coordinates) by sampling the density map; and then predicting the depth (i.e., the z coordinate) by sampling the depth values at the ray determined by each 2D point. Extensive experiments are conducted, and both quantitative and qualitative results show that our proposed approach significantly outperforms other baseline methods.","tags":[],"title":"SketchSampler: Sketch-based 3D Reconstruction via View-dependent Depth Sampling","type":"publication"},{"authors":["Yinan He","Gengshi Huang","Siyu Chen","Jianing Teng","Wang Kun","Zhenfei Yin","Lu Sheng","Ziwei Liu","Yu Qiao","Jing Shao"],"categories":[],"content":"","date":1666483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666483200,"objectID":"bc9d6d090767fcadca40fb32a6600f87","permalink":"https://lucassheng.github.io/publication/he-x-learner-2022/","publishdate":"2022-09-05T15:14:59+08:00","relpermalink":"/publication/he-x-learner-2022/","section":"publication","summary":"In computer vision, pre-training models based on largescale supervised learning have been proven effective over the past few years. However, existing works mostly focus on learning from individual task with single data source (e.g., ImageNet for classification or COCO for detection). This restricted form limits their generalizability and usability due to the lack of vast semantic information from various tasks and data sources. Here, we demonstrate that jointly learning from heterogeneous tasks and multiple data sources contributes to universal visual representation, leading to better transferring results of various downstream tasks. Thus, learning how to bridge the gaps among different tasks and data sources is the key, but it still remains an open question. In this work, we propose a representation learning framework called X-Learner, which learns the universal feature of multiple vision tasks supervised by various sources, with expansion and squeeze stage: 1) Expansion Stage: X-Learner learns the task-specific feature to alleviate task interference and enrich the representation by reconciliation layer. 2) Squeeze Stage: X-Learner condenses the model to a reasonable size and learns the universal and generalizable representation for various tasks transferring. Extensive experiments demonstrate that X-Learner achieves strong performance on different tasks without extra annotations, modalities and computational costs compared to existing representation learning methods. Notably, a single X-Learner model shows remarkable gains of 3.0%, 3.3% and 1.8% over current pretrained models on 12 downstream datasets for classification, object detection and semantic segmentation.","tags":[],"title":"X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation","type":"publication"},{"authors":["Daigang Cai","Lichen Zhao","Jing Zhang","Lu Sheng","Dong Xu"],"categories":[],"content":"","date":1665446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520747,"objectID":"670cd8a919d5bbc10a90a259948d9404","permalink":"https://lucassheng.github.io/publication/cai-3-djcg-2022/","publishdate":"2022-08-26T13:43:22.294962Z","relpermalink":"/publication/cai-3-djcg-2022/","section":"publication","summary":"Accepted by CVPR 2022, as **Oral presentation**. ","tags":[],"title":"3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds","type":"publication"},{"authors":[],"categories":[],"content":"","date":1662356280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662356280,"objectID":"c2a35fa01234626558c821da50d06dcd","permalink":"https://lucassheng.github.io/project/image-and-video-synthesis/","publishdate":"2022-09-05T13:38:00+08:00","relpermalink":"/project/image-and-video-synthesis/","section":"project","summary":"","tags":[],"title":"Image, Video and 3D Synthesis","type":"project"},{"authors":[],"categories":[],"content":"","date":1662355897,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662355897,"objectID":"069b903cf61966bb4b40f5d4abe01940","permalink":"https://lucassheng.github.io/project/language-for-3d-scenes/","publishdate":"2022-09-05T13:31:37+08:00","relpermalink":"/project/language-for-3d-scenes/","section":"project","summary":"","tags":[],"title":"Language for 3D Scenes","type":"project"},{"authors":[],"categories":[],"content":"","date":1662355840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662355840,"objectID":"45bdd89c240e8069860e112bac34f54f","permalink":"https://lucassheng.github.io/project/3d-perception-and-modeling/","publishdate":"2022-09-05T13:30:40+08:00","relpermalink":"/project/3d-perception-and-modeling/","section":"project","summary":"","tags":[],"title":"3D Perception and Modeling","type":"project"},{"authors":["Buyu Li","Yongchi Zhao","Zhelun Shi","Lu Sheng"],"categories":[],"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520748,"objectID":"26423c996418b9f54564ab187d911aff","permalink":"https://lucassheng.github.io/publication/li-danceformer-2022/","publishdate":"2022-08-26T13:43:22.430964Z","relpermalink":"/publication/li-danceformer-2022/","section":"publication","summary":"Accepted by AAAI 2022.","tags":["Domain(s) Of Application (APP)"],"title":"DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer","type":"publication"},{"authors":["Kaisiyuan Wang","Lu Sheng","Shuhang Gu","Dong Xu"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520748,"objectID":"44e3d414b8b8d001db79c58571ed6b1f","permalink":"https://lucassheng.github.io/publication/wang-vpu-2022/","publishdate":"2022-08-26T13:43:22.56475Z","relpermalink":"/publication/wang-vpu-2022/","section":"publication","summary":"In this work, we propose a new patch-based framework called VPU for the video-based point cloud upsampling task by effectively exploiting temporal dependency among multiple consecutive point cloud frames, in which each frame consists of a set of unordered, sparse and irregular 3D points. Rather than adopting the sophisticated motion estimation strategy in video analysis, we propose a new spatio-temporal aggregation (STA) module to effectively extract, align and aggregate rich local geometric clues from consecutive frames at the feature level. By more reliably summarizing spatio-temporally consistent and complementary knowledge from multiple frames in the resultant local structural features, our method better infers the local geometry distributions at the current frame. In addition, our STA module can be readily incorporated with various existing single frame-based point upsampling methods (e.g., PU-Net, MPU, PU-GAN and PU-GCN). Comprehensive experiments on multiple point cloud sequence datasets demonstrate our video-based point cloud upsampling framework achieves substantial performance improvement over its single frame-based counterparts.","tags":["Feature extraction","Point cloud sequence","point cloud upsampling","spatial-temporal aggregation"],"title":"VPU: A Video-Based Point Cloud Upsampling Framework","type":"publication"},{"authors":["Lichen Zhao","Daigang Cai","Lu Sheng","Dong Xu"],"categories":[],"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520743,"objectID":"84962c2398201f7ebf2fb554cc0ef308","permalink":"https://lucassheng.github.io/publication/zhao-3-dvg-transformer-2021/","publishdate":"2022-08-26T13:43:17.422763Z","relpermalink":"/publication/zhao-3-dvg-transformer-2021/","section":"publication","summary":"Accepted by ICCV 2021. **1st place** (up to Oct. 10, 2021) in the [ScanRefer Benchmark](http://kaldir.vc.in.tum.de/scanrefer_benchmark/), **winner** for the 3D Object Localization Challenge at the [CVPR 2021, 1st Workshop on Language for 3D Scenes](https://language3dscenes.github.io/).","tags":[],"title":"3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds","type":"publication"},{"authors":["Xiaolei Wu","Zhihao Hu","Lu Sheng","Dong Xu"],"categories":[],"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520743,"objectID":"522555cf6219374acb4ea8624eab415a","permalink":"https://lucassheng.github.io/publication/wu-styleformer-2021/","publishdate":"2022-08-26T13:43:17.5561Z","relpermalink":"/publication/wu-styleformer-2021/","section":"publication","summary":"In this work, we propose a new feed-forward arbitrary style transfer method, referred to as StyleFormer, which can simultaneously fulfill fine-grained style diversity and semantic content coherency. Specifically, our transformerinspired feature-level stylization method consists of three modules: (a) the style bank generation module for sparse but compact parametric style pattern extraction, (b) the transformer-driven style composition module for contentguided global style composition, and (c) the parametric content modulation module for flexible but faithful stylization. The output stylized images are impressively coherent with the content structure, sensitive to the detailed style variations, but still holistically adhere to the style distributions from the style images. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our StyleFormer outperforms the existing SOTA methods in generating visually plausible stylization results with real-time efficiency.","tags":[],"title":"StyleFormer: Real-time Arbitrary Style Transfer via Parametric Style Composition","type":"publication"},{"authors":["Guanze Liu","Yu Rong","Lu Sheng"],"categories":[],"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520742,"objectID":"4b6ac0656270a70919814594735f2098","permalink":"https://lucassheng.github.io/publication/liu-votehmr-2021/","publishdate":"2022-08-26T13:43:17.260728Z","relpermalink":"/publication/liu-votehmr-2021/","section":"publication","summary":"Accepted by ACM Multimedia 2021, as **Oral presentation**. ","tags":[],"title":"VoteHMR: Occlusion-Aware Voting Network for Robust 3D Human Mesh Recovery from Partial Point Clouds","type":"publication"},{"authors":["Kaisiyuan Wang,","Lu Sheng","Shuhang Gu,","Dong Xu"],"categories":[],"content":"","date":1628726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628726400,"objectID":"dc03d40e3e3123f3933b25fdf1e12e0b","permalink":"https://lucassheng.github.io/publication/wang-sequential-2021/","publishdate":"2021-08-12T00:00:00Z","relpermalink":"/publication/wang-sequential-2021/","section":"publication","summary":"In this work, we propose a new sequential point cloud upsampling method called SPU, which aims to upsample sparse, non-uniform, and orderless point cloud sequences by effectively exploiting rich and complementary temporal dependency from multiple inputs. Specifically, these inputs include a set of multi-scale short-term features from the 3D points in three consecutive frames (i.e., the previous/current/subsequent frame) and a long-term latent representation accumulated throughout the point cloud sequence. Considering that these temporal clues are not well aligned in the coordinate space, we propose a new temporal alignment module (TAM) based on the cross-attention mechanism to transform each individual feature into the feature space of the current frame. We also propose a new gating mechanism to learn the optimal weights for these transformed features, based on which the transformed features can be effectively aggregated as the final fused feature. The fused feature can be readily fed into the existing single frame-based point cloud upsampling methods (e.g., PU-Net, MPU and PU-GAN) to generate the dense point cloud for the current frame. Comprehensive experiments on three benchmark datasets DYNA, COMA, and MSR Action3D demonstrate the effectiveness of our method for upsampling point cloud sequences.","tags":[],"title":"Sequential Point Cloud Upsampling by Exploiting Multi-Scale Temporal Dependency","type":"publication"},{"authors":["Lichen Zhao,","Jinyang Guo,","Dong Xu,","Lu Sheng"],"categories":[],"content":"","date":1627948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627948800,"objectID":"5bdd40247c82d978354bcf1835bea8dd","permalink":"https://lucassheng.github.io/publication/zhao-transformer-3d-det-2021/","publishdate":"2022-09-05T14:52:30+08:00","relpermalink":"/publication/zhao-transformer-3d-det-2021/","section":"publication","summary":"Voting-based methods (e.g., VoteNet) have achieved promising results for 3D object detection. However, the simple voting operation in VoteNet may lead to less accurate voting results that are far away from the true object centers. In this work, we propose a simple but effective 3D object detection method called Transformer3D-Det (T3D), in which we additionally introduce a transformer based vote refinement module to refine the voting results of VoteNet and can thus significantly improve the 3D object detection performance. Specifically, our T3D framework consists of three modules: a vote generation module, a vote refinement module, and a bounding box generation module. Given an input point cloud, we first utilize the vote generation module to generate multiple coarse vote clusters. Then, the clustered coarse votes will be refined by using our transformer based vote refinement module to produce more accurate and meaningful votes. Finally, the bounding box generation module takes the refined vote clusters as the input and generates the final detection result for the input point cloud. To alleviate the impact of inaccurate votes, we also propose a new non-vote loss function to train our T3D. As a result, our T3D framework can achieve better 3D object detection performance. Comprehensive experiments on two benchmark datasets ScanNetV2 and SUN RGB-D demonstrate the effectiveness of our T3D framework for 3D object detection.","tags":[],"title":"Transformer3D-Det: Improving 3D Object Detection by Vote Refinement","type":"publication"},{"authors":["Bowen Cheng","Lu Sheng","Shaoshuai Shi","Ming Yang","Dong Xu"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520743,"objectID":"20a1a2d02d5c8bcd14397f3802dd379e","permalink":"https://lucassheng.github.io/publication/cheng-back-tracing-2021/","publishdate":"2022-08-26T13:43:17.683043Z","relpermalink":"/publication/cheng-back-tracing-2021/","section":"publication","summary":"Accepted by CVPR 2021. A lightweight model with SOTA results on ScanNet V2 and SUN RGB-D datasets.","tags":[],"title":"Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds","type":"publication"},{"authors":["Yinan He","Bei Gan","Siyu Chen","Yichun Zhou","Guojun Yin","Luchuan Song","Lu Sheng","Jing Shao","Ziwei Liu"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520743,"objectID":"3dd31546731053f09ad0256b0afcd0b5","permalink":"https://lucassheng.github.io/publication/he-forgerynet-2021/","publishdate":"2022-08-26T13:43:17.80941Z","relpermalink":"/publication/he-forgerynet-2021/","section":"publication","summary":"Accepted by CVPR 2021, as **Oral Presentation**. We host the [ForgeryNet Challenge 2021](https://competitions.codalab.org/competitions/33386) in [ICCV 2021, The 3rd Workshop on Sensing, Understanding and Synthesizing Humans](https://yinanhe.github.io/projects/forgerynet.html#).","tags":[],"title":"ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis","type":"publication"},{"authors":["Yandan Yang","Lu Sheng","Xiaolong Jiang","Haochen Wang","Dong Xu","Xianbin Cao"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520743,"objectID":"547c04e9e07102bf0bd50f1c331a5ad4","permalink":"https://lucassheng.github.io/publication/yang-increaco-2021/","publishdate":"2022-08-26T13:43:17.939087Z","relpermalink":"/publication/yang-increaco-2021/","section":"publication","summary":"Automatic check-out (ACO) emerges as an integral component in recent self-service retailing stores, which aims at automatically detecting and counting the randomly placed products upon a check-out platform. Existing data-driven counting works still have difﬁculties in generalizing to realworld retail product counting scenarios, since (1) real check-out images are hard to collect or cover all products and their possible layouts, (2) rapid updating of the product list leads to frequent and tedious re-training of the counting models. To overcome these obstacles, we contribute a practical automatic check-out framework tailored to real-world retail product counting scenarios, consisting of a photorealistic exemplar augmentation to generate physically reliable and photorealistic check-out images from canonical exemplars scanned for each product and an incremental learning strategy to match the updating nature of the ACO system with much fewer training effort. Through comprehensive studies, we show that the proposed IncreACO serves as an effective framework on the recent Retail Product Checkout (RPC) dataset, where the proposed photorealistic exemplar augmentation remarkably improves the counting performance against the state-of-the-art methods (77.15% v.s. 72.83% in counting accuracy), whilst the proposed incremental learning framework consistently extends the counting performance to new categories.","tags":[],"title":"IncreACO: Incrementally Learned Automatic Check-out with Photorealistic Exemplar Augmentation","type":"publication"},{"authors":["Chi Ho Cheung","Lu Sheng","King Ngi Ngan"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520743,"objectID":"f3896f32e6068707e9b3685e4065587b","permalink":"https://lucassheng.github.io/publication/cheung-motion-2021/","publishdate":"2022-08-26T13:43:18.338894Z","relpermalink":"/publication/cheung-motion-2021/","section":"publication","summary":"Due to the wide interest in advanced multimedia experience, free-viewpoint communication is being greatly developed in recent years. In the free-viewpoint communication, viewers can perceive a view from any angle and any position of a scene. Even though the preferred views are not captured, we can generate the views through virtual view synthesis that synthesizes an arbitrary view from captured reference view(s). For daily use, only one or few cameras in baseline distance are given to capture the scene that makes the virtual view synthesis challenging. The task is more difficult when the camera is continuously moving. In this paper, we propose a particle cell to model a reference view sequence to a set of moving particles for virtual view synthesis. Using our novel hybrid motion estimation scheme, the projected coordinates of particles in each frame are obtained even they are occluded. The particles are warped to a virtual view and synthesized as a virtual view sequence. Our method is applicable for both dynamic camera setting and static camera setting. The experimental results show our method outperforms the state-of-the-art algorithms in dynamic camera datasets and presents improvement in static camera datasets in general.","tags":["Three-dimensional displays","3D-warping","Cameras","depth-image-based rendering","Free-viewpoint communication","Motion compensation","Motion estimation","Optical imaging","Predictive models","Rendering (computer graphics)","virtual view synthesis"],"title":"Motion Compensated Virtual View Synthesis Using Novel Particle Cell","type":"publication"},{"authors":["Rui Su","Dong Xu","Lu Sheng","Wanli Ouyang"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520743,"objectID":"9cf0933398c31c216b8258b6e9c9a7c9","permalink":"https://lucassheng.github.io/publication/su-pcg-tal-2021/","publishdate":"2022-08-26T13:43:18.077443Z","relpermalink":"/publication/su-pcg-tal-2021/","section":"publication","summary":"There are two major lines of works, i.e., anchor-based and frame-based approaches, in the field of temporal action localization. But each line of works is inherently limited to a certain detection granularity and cannot simultaneously achieve high recall rates with accurate action boundaries. In this work, we propose a progressive cross-granularity cooperation (PCG-TAL) framework to effectively take advantage of complementarity between the anchor-based and frame-based paradigms, as well as between two-view clues (i.e., appearance and motion). Specifically, our new Anchor-Frame Cooperation (AFC) module can effectively integrate both two-granularity and two-stream knowledge at the feature and proposal levels, as well as within each AFC module and across adjacent AFC modules. Specifically, the RGB-stream AFC module and the flow-stream AFC module are stacked sequentially to form a progressive localization framework. The whole framework can be learned in an end-to-end fashion, whilst the temporal action localization performance can be gradually boosted in a progressive manner. Our newly proposed framework outperforms the state-of-the-art methods on three benchmark datasets the THUMOS14, ActivityNet v1.3 and UCF-101-24, which clearly demonstrates the effectiveness of our framework.","tags":["cross-granularity cooperation","cross-stream cooperation","Feature extraction","Spatiotemporal phenomena","Temporal action localization"],"title":"PCG-TAL: Progressive Cross-Granularity Cooperation for Temporal Action Localization","type":"publication"},{"authors":["Lu Sheng","Junting Pan","Jiaming Guo","Jing Shao","Chen Change Loy"],"categories":[],"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520744,"objectID":"d89ff34a31319e3f2164d5c3da9129b0","permalink":"https://lucassheng.github.io/publication/sheng-high-quality-2020/","publishdate":"2022-08-26T13:43:18.601071Z","relpermalink":"/publication/sheng-high-quality-2020/","section":"publication","summary":"This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle-consistent flow-based conditioned variational autoencoder to capture the long-term motion distributions, by which the learned bi-directional flows ensure the physical reliability of the predicted motions and provide explicit occlusion handling in a principled manner. Integrating structural annotations into the flow prediction also improves the structural awareness in the I2V generation process. Quantitative and qualitative evaluations over the autonomous driving and human action datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods. The code has been released: https://github.com/junting/seg2vid.","tags":["Conditioned generative model","Image and video synthesis","Motion prediction and estimatiovn","Unsupervised learning"],"title":"High-Quality Video Generation from Static Structural Annotations","type":"publication"},{"authors":["Ronghao Guo","Chen Lin","Chuming Li","Keyu Tian","Ming Sun","Lu Sheng","Junjie Yan"],"categories":[],"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520744,"objectID":"1e135fb7a08d2b9a5567da93b99cb1eb","permalink":"https://lucassheng.github.io/publication/vedaldi-powering-2020/","publishdate":"2022-08-26T13:43:18.468838Z","relpermalink":"/publication/vedaldi-powering-2020/","section":"publication","summary":"One-shot NAS method has attracted much interest from the research community due to its remarkable training eﬃciency and capacity to discover high performance models. However, the search spaces of previous one-shot based works usually relied on hand-craft design and were short for ﬂexibility on the network topology. In this work, we try to enhance the one-shot NAS by exploring high-performing network architectures in our large-scale Topology Augmented Search Space (i.e, over 3.4 × 1010 diﬀerent topological structures). Speciﬁcally, the diﬃculties for architecture searching in such a complex space has been eliminated by the proposed stabilized share-parameter proxy, which employs Stochastic Gradient Langevin Dynamics to enable fast shared parameter sampling, so as to achieve stabilized measurement of architecture performance even in search space with complex topological structures. The proposed method, namely Stablized Topological Neural Architecture Search (ST-NAS), achieves state-of-the-art performance under Multiply-Adds (MAdds) constraint on ImageNet. Our lite model ST-NAS-A achieves 76.4% top-1 accuracy with only 326M MAdds. Our moderate model STNAS-B achieves 77.9% top-1 accuracy just required 503M MAdds. Both of our models oﬀer superior performances in comparison to other concurrent works on one-shot NAS.","tags":[],"title":"Powering One-Shot Topological NAS with Stabilized Share-Parameter Proxy","type":"publication"},{"authors":["Yuyang Qian","Guojun Yin","Lu Sheng","Zixuan Chen","Jing Shao"],"categories":[],"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520743,"objectID":"819af01444060ebded6d7406b4472521","permalink":"https://lucassheng.github.io/publication/vedaldi-thinking-2020/","publishdate":"2022-08-26T13:43:18.206918Z","relpermalink":"/publication/vedaldi-thinking-2020/","section":"publication","summary":"As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We ﬁnd that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F3-Net), taking advantages of two diﬀerent but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F3-Net signiﬁcantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media.","tags":[],"title":"Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues","type":"publication"},{"authors":["Minghua Liu","Lu Sheng","Sheng Yang","Jing Shao","Shi-Min Hu"],"categories":[],"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520744,"objectID":"33d944588f85e69c4c01298886bea42c","permalink":"https://lucassheng.github.io/publication/liu-morphing-2020/","publishdate":"2022-08-26T13:43:18.749532Z","relpermalink":"/publication/liu-morphing-2020/","section":"publication","summary":"3D point cloud completion, the task of inferring the complete geometric shape from a partial point cloud, has been attracting attention in the community. For acquiring high-fidelity dense point clouds and avoiding uneven distribution, blurred details, or structural loss of existing methods' results, we propose a novel approach to complete the partial point cloud in two stages. Specifically, in the first stage, the approach predicts a complete but coarse-grained point cloud with a collection of parametric surface elements. Then, in the second stage, it merges the coarse-grained prediction with the input point cloud by a novel sampling algorithm. Our method utilizes a joint loss function to guide the distribution of the points. Extensive experiments verify the effectiveness of our method and demonstrate that it outperforms the existing methods in both the Earth Mover's Distance (EMD) and the Chamfer Distance (CD).","tags":[],"title":"Morphing and Sampling Network for Dense Point Cloud Completion","type":"publication"},{"authors":["Bowen Dong","Lu Sheng"],"categories":[],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520744,"objectID":"a90f50d8477e506b6bcf67e24fa01c55","permalink":"https://lucassheng.github.io/publication/dong-bags-2019/","publishdate":"2022-08-26T13:43:19.146767Z","relpermalink":"/publication/dong-bags-2019/","section":"publication","summary":"* *Background*: Based on the seminal work proposed by Zhou et al., much of the recent progress in learning monocular visual odometry, i. e. depth and camera motion from monocular videos, can be attributed to the tricks in the training procedure, such as data augmentation and learning objectives.\n* *Methods*: Herein, we categorize a collection of such tricks through the theoretical examination and empirical evaluation of their effects on the final accuracy of the visual odometry.\n* *Results/Conclusions*: By combining the aforementioned tricks, we were able to significantly improve a baseline model adapted from SfMLearner without additional inference costs. Furthermore, we analyzed the principles of these tricks and the reason for their success. Practical guidelines for future research are also presented.\"","tags":[],"title":"Bags of tricks for learning depth and camera motion from monocular videos","type":"publication"},{"authors":["Zihao Wang","Xihui Liu","Hongsheng Li","Lu Sheng","Junjie Yan","Xiaogang Wang","Jing Shao"],"categories":[],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520744,"objectID":"f55e66ed1ee4dd0d44614fb53f0e9946","permalink":"https://lucassheng.github.io/publication/wang-camp-2019/","publishdate":"2022-08-26T13:43:19.272605Z","relpermalink":"/publication/wang-camp-2019/","section":"publication","summary":"Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach.","tags":[],"title":"CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval","type":"publication"},{"authors":["Chufeng Tang","Lu Sheng","Zhao-Xiang Zhang","Xiaolin Hu"],"categories":[],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520744,"objectID":"b7c6da6357c3d6cbc2ff7293a402160f","permalink":"https://lucassheng.github.io/publication/tang-improving-2019/","publishdate":"2022-08-26T13:43:19.01174Z","relpermalink":"/publication/tang-improving-2019/","section":"publication","summary":"Pedestrian attribute recognition has been an emerging research topic in the area of video surveillance. To predict the existence of a particular attribute, it is demanded to localize the regions related to the attribute. However, in this task, the region annotations are not available. How to carve out these attribute-related regions remains challenging. Existing methods applied attribute-agnostic visual attention or heuristic body-part localization mechanisms to enhance the local feature representations, while neglecting to employ attributes to deﬁne local feature areas. We propose a ﬂexible Attribute Localization Module (ALM) to adaptively discover the most discriminative regions and learns the regional features for each attribute at multiple levels. Moreover, a feature pyramid architecture is also introduced to enhance the attribute-speciﬁc localization at low-levels with high-level semantic guidance. The proposed framework does not require additional region annotations and can be trained end-to-end with multi-level deep supervision. Extensive experiments show that the proposed method achieves state-of-the-art results on three pedestrian attribute datasets, including PETA, RAP, and PA-100K.","tags":[],"title":"Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization","type":"publication"},{"authors":["Lu Sheng","Dan Xu","Wanli Ouyang","Xiaogang Wang"],"categories":[],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520744,"objectID":"48d7a79e3c56960d8bed32b74e4f5760","permalink":"https://lucassheng.github.io/publication/sheng-unsupervised-2019/","publishdate":"2022-08-26T13:43:18.878047Z","relpermalink":"/publication/sheng-unsupervised-2019/","section":"publication","summary":"In this paper we tackle the joint learning problem of keyframe detection and visual odometry towards monocular visual SLAM systems. As an important task in visual SLAM, keyframe selection helps efﬁcient camera relocalization and effective augmentation of visual odometry. To beneﬁt from it, we ﬁrst present a deep network design for the keyframe selection, which is able to reliably detect keyframes and localize new frames, then an end-to-end unsupervised deep framework further proposed for simultaneously learning the keyframe selection and the visual odometry tasks. As far as we know, it is the ﬁrst work to jointly optimize these two complementary tasks in a single deep framework. To make the two tasks facilitate each other in the learning, a collaborative optimization loss based on both geometric and visual metrics is proposed. Extensive experiments on publicly available datasets (i.e. KITTI raw dataset and its odometry split [12]) clearly demonstrate the effectiveness of the proposed approach, and new state-ofthe-art results are established on the unsupervised depth and pose estimation from monocular video.","tags":[],"title":"Unsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM","type":"publication"},{"authors":["Fanzi Wu","Songnan Li","Tianhao Zhao","King Ngi Ngan","Lu Sheng"],"categories":[],"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520745,"objectID":"c801131e7a073613749b83af6fa957d9","permalink":"https://lucassheng.github.io/publication/wu-cascaded-2019/","publishdate":"2022-08-26T13:43:19.405503Z","relpermalink":"/publication/wu-cascaded-2019/","section":"publication","summary":"This paper proposes a novel model fitting algorithm for 3D facial expression reconstruction from a single image. Face expression reconstruction from a single image is a challenging task in computer vision. Most state-of-the-art methods fit the input image to a 3D Morphable Model (3DMM). These methods need to solve a stochastic problem and cannot deal with expression and pose variations. To solve this problem, we adopt a 3D face expression model and use a combined feature which is robust to scale, rotation and different lighting conditions. The proposed method applies a cascaded regression framework to estimate parameters for the 3DMM. 2D landmarks are detected and used to initialize the 3D shape and mapping matrices. In each iteration, residues between the current 3DMM parameters and the ground truth are estimated and then used to update the 3D shapes. The mapping matrices are also calculated based on the updated shapes and 2D landmarks. HOG features of the local patches and displacements between 3D landmark projections and 2D landmarks are exploited. Compared with existing methods, the proposed method is robust to expression and pose changes and can reconstruct higher fidelity 3D face shape.","tags":["3DMM","Cascaded regression","Facial expression","Landmarks"],"title":"Cascaded regression using landmark displacement for 3D face reconstruction","type":"publication"},{"authors":["Guojun Yin","Lu Sheng","Bin Liu","Nenghai Yu","Xiaogang Wang","Jing Shao"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520745,"objectID":"586eb15f2c33a2ee8a24a9976bb995f0","permalink":"https://lucassheng.github.io/publication/yin-context-2019/","publishdate":"2022-08-26T13:43:19.926477Z","relpermalink":"/publication/yin-context-2019/","section":"publication","summary":"Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to the state-of-the-art methods.","tags":[],"title":"Context and Attribute Grounded Dense Captioning","type":"publication"},{"authors":["Buyu Li","Wanli Ouyang","Lu Sheng","Xingyu Zeng","Xiaogang Wang"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520745,"objectID":"82291216c04ca8a92ef1bcec1bc072a2","permalink":"https://lucassheng.github.io/publication/li-gs-3-d-2019/","publishdate":"2022-08-26T13:43:19.666064Z","relpermalink":"/publication/li-gs-3-d-2019/","section":"publication","summary":"We present an efﬁcient 3D object detection framework based on a single RGB image in the scenario of autonomous driving. Our efforts are put on extracting the underlying 3D information in a 2D image and determining the accurate 3D bounding box of the object without point cloud or stereo data. Leveraging the off-the-shelf 2D object detector, we propose an artful approach to efﬁciently obtain a coarse cuboid for each predicted 2D box. The coarse cuboid has enough accuracy to guide us to determine the 3D box of the object by reﬁnement. In contrast to previous state-ofthe-art methods that only use the features extracted from the 2D bounding box for box reﬁnement, we explore the 3D structure information of the object by employing the visual features of visible surfaces. The new features from surfaces are utilized to eliminate the problem of representation ambiguity brought by only using a 2D bounding box. Moreover, we investigate different methods of 3D box reﬁnement and discover that a classiﬁcation formulation with quality aware loss has much better performance than regression. Evaluated on the KITTI benchmark, our approach outperforms current state-of-the-art methods for single RGB image based 3D object detection.","tags":[],"title":"GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving","type":"publication"},{"authors":["Guojun Yin","Bin Liu","Lu Sheng","Nenghai Yu","Xiaogang Wang","Jing Shao"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520746,"objectID":"46bc0d9abda1e17147c88ab5cf94a89f","permalink":"https://lucassheng.github.io/publication/yin-semantics-2019/","publishdate":"2022-08-26T13:43:20.851947Z","relpermalink":"/publication/yin-semantics-2019/","section":"publication","summary":"Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods.","tags":["Image and Video Synthesis","Vision + Language"],"title":"Semantics Disentangling for Text-To-Image Generation","type":"publication"},{"authors":["Junting Pan","Chengyu Wang","Xu Jia","Jing Shao","Lu Sheng","Junjie Yan","Xiaogang Wang"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520745,"objectID":"199c5ba9f4e413d484de33706ab602e5","permalink":"https://lucassheng.github.io/publication/pan-video-2019/","publishdate":"2022-08-26T13:43:19.533784Z","relpermalink":"/publication/pan-video-2019/","section":"publication","summary":"This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between ﬂexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difﬁcult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the ﬁrst frame. Then we animate the scene based on its semantic meaning to obtain temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical ﬂow as a beneﬁcial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the ﬂow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods. The source code will be released on https://github.com/junting/seg2vid.","tags":[],"title":"Video Generation From Single Semantic Label Map","type":"publication"},{"authors":["Lu Sheng","Jianfei Cai","Tat-Jen Cham","Vladimir Pavlovic","King Ngi Ngan"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520745,"objectID":"e0a6e827b676fc17db8f465b3cc660d1","permalink":"https://lucassheng.github.io/publication/sheng-visibility-2019/","publishdate":"2022-08-26T13:43:19.796148Z","relpermalink":"/publication/sheng-visibility-2019/","section":"publication","summary":"In this paper, we propose a generative framework that unifies depth-based 3D facial pose tracking and face model adaptation on-the-fly, in the unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Specifically, we introduce a statistical 3D morphable model that flexibly describes the distribution of points on the surface of the face model, with an efficient switchable online adaptation that gradually captures the identity of the tracked subject and rapidly constructs a suitable face model when the subject changes. Moreover, unlike prior art that employed ICP-based facial pose estimation, to improve robustness to occlusions, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility with respect to the input point cloud. Ablation studies and experimental results on Biwi and ICT-3DHP datasets demonstrate that the proposed framework is effective and outperforms completing state-of-the-art depth-based methods.","tags":["3D facial pose tracking","Adaptation models","depth","generative model","mixture of Gaussian models","online Bayesian model","Pose estimation"],"title":"Visibility Constrained Generative Model for Depth-Based 3D Facial Pose Tracking","type":"publication"},{"authors":["Yongcheng Liu","Lu Sheng","Jing Shao","Junjie Yan","Shiming Xiang","Chunhong Pan"],"categories":[],"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520745,"objectID":"c6c5740d304e4f2951793caeb5e8b3cc","permalink":"https://lucassheng.github.io/publication/liu-multi-label-2018/","publishdate":"2022-08-26T13:43:20.058921Z","relpermalink":"/publication/liu-multi-label-2018/","section":"publication","summary":"Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.","tags":["knowledge distillation","multi-label image classification","weakly-supervised detection"],"title":"Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection","type":"publication"},{"authors":["Guojun Yin","Lu Sheng","Bin Liu","Nenghai Yu","Xiaogang Wang","Jing Shao","Chen Change Loy"],"categories":[],"content":"","date":1538092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520745,"objectID":"a5243ef6d3ce5c7d2335d622fcb040e9","permalink":"https://lucassheng.github.io/publication/ferrari-zoom-net-2018/","publishdate":"2022-08-26T13:43:20.189458Z","relpermalink":"/publication/ferrari-zoom-net-2018/","section":"publication","summary":"Recognizing visual relationships subject-predicate-object among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features. The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our ﬁnal Zoom-Net. We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the eﬀectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 11.42% from 8.16%) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin.","tags":[],"title":"Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition","type":"publication"},{"authors":["Lu Sheng","Ziyi Lin","Jing Shao","Xiaogang Wang"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520746,"objectID":"64a7b4a82b4ba40bf4002e7e81bee3fb","permalink":"https://lucassheng.github.io/publication/sheng-avatar-net-2018/","publishdate":"2022-08-26T13:43:20.718852Z","relpermalink":"/publication/sheng-avatar-net-2018/","section":"publication","summary":"Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efﬁciency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efﬁcient yet effective Avatar-Net that enables visually plausible multiscale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multiscale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efﬁciency of the proposed method in generating high-quality stylized images, with a series of successive applications include multiple style integration, video stylization and etc.","tags":[],"title":"Avatar-Net: Multi-scale Zero-Shot Style Transfer by Feature Decoration","type":"publication"},{"authors":["Yu Liu","Fangyin Wei","Jing Shao","Lu Sheng","Junjie Yan","Xiaogang Wang"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520746,"objectID":"97a6fdf77c4463b9fcbafccdee1fb06a","permalink":"https://lucassheng.github.io/publication/liu-exploring-2018/","publishdate":"2022-08-26T13:43:20.583637Z","relpermalink":"/publication/liu-exploring-2018/","section":"publication","summary":"This paper proposes learning disentangled but complementary face features with a minimal supervision by face identiﬁcation. Speciﬁcally, we construct an identity Distilling and Dispelling Autoencoder (D2AE) framework that adversarially learns the identity-distilled features for identity veriﬁcation and the identity-dispelled features to fool the veriﬁcation system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only preserve state-of-the-art identity veriﬁcation performance on LFW, but also acquire comparable discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.","tags":[],"title":"Exploring Disentangled Feature Representation Beyond Face Identification","type":"publication"},{"authors":["Shuyang Sun","Zhanghui Kuang","Lu Sheng","Wanli Ouyang","Wei Zhang"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520745,"objectID":"60c1cd69609795fb4b62584a012e5e3f","permalink":"https://lucassheng.github.io/publication/sun-optical-2018/","publishdate":"2022-08-26T13:43:20.32152Z","relpermalink":"/publication/sun-optical-2018/","section":"publication","summary":"Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatio-temporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatiotemporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96.0% and 74.2% accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at: https://github.com/kevin-ssy/Optical-Flow-Guided-Feature","tags":[],"title":"Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition","type":"publication"},{"authors":["Chi Ho Cheung","King Ngi Ngan","Lu Sheng"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520746,"objectID":"36c00939970ccd9884f266286c77f01a","permalink":"https://lucassheng.github.io/publication/cheung-spatio-temporal-2018/","publishdate":"2022-08-26T13:43:20.454348Z","relpermalink":"/publication/cheung-spatio-temporal-2018/","section":"publication","summary":"Depth image-based rendering is an important technique for virtual view synthesis with limited 3-D data. However, occluded areas result in disocclusions in synthesized images. Filling of the disocclusions in a plausible manner is a critical task in virtual view synthesis. In addition to spatial consistency, temporal consistency of the filled regions also affects the visual quality. In this paper, we propose a novel codebook method called sprite cell for filling the disocclusions with high spatial and temporal consistency. Each codeword consists of a color vector, depth value, frame log, and the confidence score of the corresponding pixel. In contrast with the existing methods that reuse the filling results of previous frames without considering their accuracy, the proposed method estimates the confidence scores of the filling results to prevent temporal continuation of filling errors. Moreover, we introduce a method to correct the luminance of filled disocclusions that compensates for the change of scenes. The experimental results show that the proposed method achieves both objective and subjective improvements over the state-of-the-art methods. The sequences synthesized with the proposed method have higher spatio-temporal consistency.","tags":["Cameras","Rendering (computer graphics)","3-D","depth image-based rendering","Disocclusion filling","Distortion","Filling","Image color analysis","inpainting","Sprites (computer)"],"title":"Spatio-Temporal Disocclusion Filling Using Novel Sprite Cells","type":"publication"},{"authors":["Xihui Liu","Haiyu Zhao","Maoqing Tian","Lu Sheng","Jing Shao","Shuai Yi","Junjie Yan","Xiaogang Wang"],"categories":[],"content":"","date":1506816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520746,"objectID":"843e6d00ae1011c75bef4f4720bfdf88","permalink":"https://lucassheng.github.io/publication/liu-hydraplus-net-2017/","publishdate":"2022-08-26T13:43:20.979779Z","relpermalink":"/publication/liu-hydraplus-net-2017/","section":"publication","summary":"Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attentionbased deep neural network, named as HydraPlus-Net (HPnet), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person reidentification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-theart methods on various datasets.","tags":[],"title":"HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis","type":"publication"},{"authors":["Lu Sheng","Jianfei Cai","Tat-Jen Cham","Vladimir Pavlovic","King Ngi Ngan"],"categories":[],"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520746,"objectID":"2d23782a5fe9783faa4c82b52cb6a003","permalink":"https://lucassheng.github.io/publication/sheng-generative-2017/","publishdate":"2022-08-26T13:43:21.109347Z","relpermalink":"/publication/sheng-generative-2017/","section":"publication","summary":"We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that uniﬁes pose tracking and face model adaptation on-the-ﬂy. Particularly, we propose a statistical 3D face model that owns the ﬂexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face model’s visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.","tags":[],"title":"A Generative Model for Depth-Based Robust 3D Facial Pose Tracking","type":"publication"},{"authors":["Songnan Li","King Ngi Ngan","Raveendran Paramesran","Lu Sheng"],"categories":[],"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520746,"objectID":"619c340ba976dc284148b22184b1977f","permalink":"https://lucassheng.github.io/publication/li-real-time-2016/","publishdate":"2022-08-26T13:43:21.237763Z","relpermalink":"/publication/li-real-time-2016/","section":"publication","summary":"We propose a real-time method to accurately track the human head pose in the 3-dimensional (3D) world. Using a RGB-Depth camera, a face template is reconstructed by fitting a 3D morphable face model, and the head pose is determined by registering this user-specific face template to the input depth video.","tags":["Three-dimensional displays","Cameras","Face","deformable face model","Head pose tracking","Image reconstruction","iterative closest point","RGB-Depth camera","Tracking"],"title":"Real-Time Head Pose Tracking with Online Face Template Reconstruction","type":"publication"},{"authors":["Lu Sheng","King Ngi Ngan","Chern-Loon Lim","Songnan Li"],"categories":[],"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520747,"objectID":"e775e13a2ea7924dd7645c37a0971eed","permalink":"https://lucassheng.github.io/publication/sheng-online-2015/","publishdate":"2022-08-26T13:43:21.366956Z","relpermalink":"/publication/sheng-online-2015/","section":"publication","summary":"In this paper, we propose a new method to online enhance the quality of a depth video based on the intermediary of a so-called static structure of the captured scene. The static and dynamic regions of the input depth frame are robustly separated by a layer assignment procedure, in which the dynamic part stays in the front while the static part fits and helps to update this structure by a novel online variational generative model with added spatial refinement. The dynamic content is enhanced spatially while the static region is otherwise substituted by the updated static structure so as to favor the long-range spatio-temporal enhancement. The proposed method both performs long-range temporal consistency on the static region and keeps necessary depth variations in the dynamic content. Thus, it can produce flicker-free and spatially optimized depth videos with reduced motion blur and depth distortion. Our experimental results reveal that the proposed method is effective in both static and dynamic indoor scenes and is compatible with depth videos captured by Kinect and time-of-flight camera. We also demonstrate that excellent performance can be achieved by the proposed method in comparison with the existing spatio-temporal approaches. In addition, our enhanced depth videos and static structures can act as effective cues to improve various applications, including depth-aided background subtraction and novel view synthesis, showing satisfactory results with few visual artifacts.","tags":["Three-dimensional displays","Dynamics","Image processing","layer assignment","online estimation","Robustness","Static structure","Streaming media","temporally consistent depth video enhancement","video enhancement"],"title":"Online Temporally Consistent Indoor Depth Video Enhancement via Static Structure","type":"publication"},{"authors":["Chi Ho Cheung","Lu Sheng","King Ngi Ngan"],"categories":[],"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520747,"objectID":"fb97f99f0f6c350a222ecf4f277d1782","permalink":"https://lucassheng.github.io/publication/cheung-disocclusion-2015/","publishdate":"2022-08-26T13:43:22.161989Z","relpermalink":"/publication/cheung-disocclusion-2015/","section":"publication","summary":"Depth image based rendering (DIBR) is an important technique to generate virtual view images with limited 3-D data. However, disocclusion is a critical problem that the regions occluded by foreground objects become visible in virtual views, which is difficult to be visual-plausibly inferred. In this paper, we propose a novel temporally consistent filling method using multiple sprites with depth (MSD) to fill the disocclusions faithfully. MSD stores the background as well as intermediate foreground objects of past frames in multiple sprites, so that amounts of pixels in disocclusions can be recovered from MSD. Moreover, we also introduce a method to recognize the source space for exemplar based inpainting to fill the remaining disocclusions. The experimental results show the proposed method achieves objective and subjective improvement compared with the state-of-the-art methods. The synthesized sequences by the proposed method have higher spatio-temporal consistency.","tags":["Three-dimensional displays","Rendering (computer graphics)","3-D","Disocclusion filling","Filling","Image color analysis","inpainting","Sprites (computer)","Robustness","depth image based rendering","multiple sprites with depth"],"title":"A disocclusion filling method using multiple sprites with depth for virtual view synthesis","type":"publication"},{"authors":["Lu Sheng","King Ngi Ngan","Tak-Wai Hui"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520747,"objectID":"cb15e15cca113be09f9741c5853fd864","permalink":"https://lucassheng.github.io/publication/sheng-accelerating-2015/","publishdate":"2022-08-26T13:43:21.501456Z","relpermalink":"/publication/sheng-accelerating-2015/","section":"publication","summary":"Various image filters for applications in the area of computer vision require the properties of the local statistics of the input image, which are always defined by the local distribution or histogram. But the huge expense of computing the distribution hampers the popularity of these filters in real-time or interactive-rate systems. In this paper, we present an efficient and practical method to estimate the local weighted distribution for the weighted median/mode filters based on the kernel density estimation with a new separable kernel defined by a weighted combinations of a series of probabilistic generative models. It reduces the large number of filtering operations in previous constant time algorithms [1, 2] to a small amount, which is also adaptive to the structure of the input image. The proposed accelerated weighted median/mode filters are effective and efficient for a variety of applications, which have comparable performance against the current state-of-the-art counterparts and cost only a fraction of their execution time.","tags":["Bilateral Filter","Input Image","Kernel Density Estimation","Local Distribution","Local Histogram"],"title":"Accelerating the Distribution Estimation for the Weighted Median/Mode Filters","type":"publication"},{"authors":["Songnan Li","King Ngi Ngan","Lu Sheng"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520747,"objectID":"248a88e2057e28fd82ce8dc3552f9d61","permalink":"https://lucassheng.github.io/publication/li-screen-camera-2014/","publishdate":"2022-08-26T13:43:21.774429Z","relpermalink":"/publication/li-screen-camera-2014/","section":"publication","summary":"In this paper, we propose a novel screen-camera calibration algorithm which aims to locate the position of the screen in the camera coordinate system. The difficulty comes from the fact that the screen is not directly visible to the camera. Rather than using an external camera or a portable mirror like in previous studies, we propose to use a more accessible and cheaper calibrating object, i.e., a thread. The thread is manipulated so that our algorithm can infer the perspective projections of the four screen corners on the image plane. The 3-dimentional (3D) position of each screen corner is then determined by minimizing the sum of squared projection errors. Experiments show that compared with the previous studies our method can generate similar calibration results without the additional hardware.","tags":["Three-dimensional displays","Cameras","Calibration","Hough line detection","RANSAC algorithm","Screen-camera calibration"],"title":"Screen-camera calibration using a thread","type":"publication"},{"authors":["Lu Sheng","King Ngi Ngan","Songnan Li"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520747,"objectID":"85b03059b55725243cece24a10bcaadc","permalink":"https://lucassheng.github.io/publication/sheng-temporal-2014/","publishdate":"2022-08-26T13:43:21.63288Z","relpermalink":"/publication/sheng-temporal-2014/","section":"publication","summary":"Depth video enhancement is an essential preprocessing step for various 3D applications. Despite extensive studies of spatial enhancement, effective temporal enhancement that both strengthens temporal consistency and keeps correct depth variation needs further research. In this paper, we propose a novel method to enhance the depth video by blending raw depth frame with the estimated intrinsic static structure, which defines static structure of captured scene and is estimated iteratively by a probabilistic generative model with sequentially incoming depth frames. Our experimental results show that the proposed method is effective both in static and dynamic scene and is compatible with various kinds of depth videos. We will demonstrate that superior performance can be achieved in comparison with existing temporal enhancement approaches.","tags":["Approximation methods","depth video enhancement","probabilistic model","temporal enhancement","variational approximation"],"title":"Temporal depth video enhancement based on intrinsic static structure","type":"publication"},{"authors":["Lu Sheng","King Ngi Ngan"],"categories":[],"content":"","date":1377993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520747,"objectID":"cb3ef97f516acec936509bc16ffe14d2","permalink":"https://lucassheng.github.io/publication/sheng-depth-2013/","publishdate":"2022-08-26T13:43:21.904691Z","relpermalink":"/publication/sheng-depth-2013/","section":"publication","summary":"Depth map is a crucial component in various 3D applications. However, current available depth maps usually suffer from low resolution, high noise, random and structural depth missing problems due to theoretical, systematic or hardware limitations. In this paper, we propose a novel method to enhance depth map with the guidance of aligned color image, tackling these problems in a whole framework, where a hybrid strategy on filling hole geometrically by the combination of joint bilateral filtering and segment-based surface structure propagation is introduced. Our experimental results prove the proposed method outperforms existing methods.","tags":["depth enhancement","Depth map","filtering","hole filling","segmentation"],"title":"Depth enhancement based on hybrid geometric hole filling strategy","type":"publication"},{"authors":["Songnan Li","King Ngi Ngan","Lu Sheng"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661520747,"objectID":"da0ce4ff90e834c468caede9573bc823","permalink":"https://lucassheng.github.io/publication/li-head-2013/","publishdate":"2022-08-26T13:43:22.034339Z","relpermalink":"/publication/li-head-2013/","section":"publication","summary":"In this paper, a fast head pose tracking system is introduced. It uses iterative closest point algorithm to register a dense face template to depth data captured by Kinect. It can achieve 33fps processing speed without specific optimization. To improve tracking robustness, head movement prediction is applied. We propose a novel scheme that can train several simple predictors together, enhancing the overall prediction accuracy. Experimental results confirm its effectiveness for head movement prediction.","tags":["head movement prediction","head pose tracking","iterative closest point algorithm","K-means like retraining"],"title":"A Head Pose Tracking System Using RGB-D Camera","type":"publication"}]