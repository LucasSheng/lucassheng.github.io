
@inproceedings{liu_votehmr_2021,
	address = {Virtual Event China},
	title = {{VoteHMR}: {Occlusion}-{Aware} {Voting} {Network} for {Robust} {3D} {Human} {Mesh} {Recovery} from {Partial} {Point} {Clouds}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {{VoteHMR}},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475309},
	doi = {10.1145/3474085.3475309},
	abstract = {3D human mesh recovery from point clouds is essential for various tasks, including AR/VR and human behavior understanding. Previous works in this field either require high-quality 3D human scans or sequential point clouds, which cannot be easily applied to low-quality 3D scans captured by consumer-level depth sensors. In this paper, we make the first attempt to reconstruct reliable 3D human shapes from single-frame partial point clouds. To achieve this, we propose an end-to-end learnable method, named VoteHMR. The core of VoteHMR is a novel occlusion-aware voting network that can first reliably produce visible joint-level features from the input partial point clouds, and then complete the joint-level features through the kinematic tree of the human skeleton. Compared with holistic features used by previous works, the joint-level features can not only effectively encode the human geometry information but also be robust to noisy inputs with self-occlusions and missing areas. By exploiting the rich complementary clues from the joint-level features and global features from the input point clouds, the proposed method encourages reliable and disentangled parameter predictions for statistical 3D human models, such as SMPL. The proposed method achieves state-of-the-art performances on two large-scale datasets, namely SURREAL and DFAUST. Furthermore, VoteHMR also demonstrates superior generalization ability on real-world datasets, such as Berkeley MHAD.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Liu, Guanze and Rong, Yu and Sheng, Lu},
	month = oct,
	year = {2021},
	pages = {955--964},
	file = {Liu et al. - 2021 - VoteHMR Occlusion-Aware Voting Network for Robust.pdf:/Users/lucasjing/Zotero/storage/WK5875Z8/Liu et al. - 2021 - VoteHMR Occlusion-Aware Voting Network for Robust.pdf:application/pdf},
}

@inproceedings{zhao_3dvg-transformer_2021,
	address = {Montreal, QC, Canada},
	title = {{3DVG}-{Transformer}: {Relation} {Modeling} for {Visual} {Grounding} on {Point} {Clouds}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{3DVG}-{Transformer}},
	url = {https://ieeexplore.ieee.org/document/9711334/},
	doi = {10.1109/ICCV48922.2021.00292},
	abstract = {Visual grounding on 3D point clouds is an emerging vision and language task that benefits various applications in understanding the 3D visual world. By formulating this task as a grounding-by-detection problem, lots of recent works focus on how to exploit more powerful detectors and comprehensive language features, but (1) how to model complex relations for generating context-aware object proposals and (2) how to leverage proposal relations to distinguish the true target object from similar proposals are not fully studied yet. Inspired by the well-known transformer architecture, we propose a relation-aware visual grounding method on 3D point clouds, named as 3DVGTransformer, to fully utilize the contextual clues for relationenhanced proposal generation and cross-modal proposal disambiguation, which are enabled by a newly designed coordinate-guided contextual aggregation (CCA) module in the object proposal generation stage, and a multiplex attention (MA) module in the cross-modal feature fusion stage. We validate that our 3DVG-Transformer outperforms the state-of-the-art methods by a large margin, on two point cloud-based visual grounding datasets, ScanRefer and Nr3D/Sr3D from ReferIt3D, especially for complex scenarios containing multiple objects of the same category.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhao, Lichen and Cai, Daigang and Sheng, Lu and Xu, Dong},
	month = oct,
	year = {2021},
	pages = {2908--2917},
	file = {Zhao et al. - 2021 - 3DVG-Transformer Relation Modeling for Visual Gro.pdf:/Users/lucasjing/Zotero/storage/X4UGNFNM/Zhao et al. - 2021 - 3DVG-Transformer Relation Modeling for Visual Gro.pdf:application/pdf},
}

@inproceedings{wu_styleformer_2021,
	address = {Montreal, QC, Canada},
	title = {{StyleFormer}: {Real}-time {Arbitrary} {Style} {Transfer} via {Parametric} {Style} {Composition}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{StyleFormer}},
	url = {https://ieeexplore.ieee.org/document/9711417/},
	doi = {10.1109/ICCV48922.2021.01435},
	abstract = {In this work, we propose a new feed-forward arbitrary style transfer method, referred to as StyleFormer, which can simultaneously fulfill fine-grained style diversity and semantic content coherency. Specifically, our transformerinspired feature-level stylization method consists of three modules: (a) the style bank generation module for sparse but compact parametric style pattern extraction, (b) the transformer-driven style composition module for contentguided global style composition, and (c) the parametric content modulation module for flexible but faithful stylization. The output stylized images are impressively coherent with the content structure, sensitive to the detailed style variations, but still holistically adhere to the style distributions from the style images. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our StyleFormer outperforms the existing SOTA methods in generating visually plausible stylization results with real-time efficiency.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wu, Xiaolei and Hu, Zhihao and Sheng, Lu and Xu, Dong},
	month = oct,
	year = {2021},
	pages = {14598--14607},
	file = {Wu et al. - 2021 - StyleFormer Real-time Arbitrary Style Transfer vi.pdf:/Users/lucasjing/Zotero/storage/AD4I8V8M/Wu et al. - 2021 - StyleFormer Real-time Arbitrary Style Transfer vi.pdf:application/pdf},
}

@inproceedings{cheng_back-tracing_2021,
	address = {Nashville, TN, USA},
	title = {Back-tracing {Representative} {Points} for {Voting}-based {3D} {Object} {Detection} in {Point} {Clouds}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578877/},
	doi = {10.1109/CVPR46437.2021.00885},
	abstract = {3D object detection in point clouds is a challenging vision task that benefits various applications for understanding the 3D visual world. Lots of recent research focuses on how to exploit end-to-end trainable Hough voting for generating object proposals. However, the current voting strategy can only receive partial votes from the surfaces of potential objects together with severe outlier votes from the cluttered backgrounds, which hampers full utilization of the information from the input point clouds. Inspired by the back-tracing strategy in the conventional Hough voting methods, in this work, we introduce a new 3D object detection method, named as Back-tracing Representative Points Network (BRNet), which generatively back-traces the representative points from the vote centers and also revisits complementary seed points around these generated points, so as to better capture the fine local structural features surrounding the potential objects from the raw point clouds. Therefore, this bottom-up and then top-down strategy in our BRNet enforces mutual consistency between the predicted vote centers and the raw surface points and thus achieves more reliable and flexible object localization and class prediction results. Our BRNet is simple but effective, which significantly outperforms the state-of-the-art methods on two large-scale point cloud datasets, ScanNet V2 (+7.5\% in terms of mAP@0.50) and SUN RGB-D (+4.7\% in terms of mAP@0.50), while it is still lightweight and efficient.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cheng, Bowen and Sheng, Lu and Shi, Shaoshuai and Yang, Ming and Xu, Dong},
	month = jun,
	year = {2021},
	pages = {8959--8968},
	file = {Cheng et al. - 2021 - Back-tracing Representative Points for Voting-base.pdf:/Users/lucasjing/Zotero/storage/HF8GYWN9/Cheng et al. - 2021 - Back-tracing Representative Points for Voting-base.pdf:application/pdf},
}

@inproceedings{he_forgerynet_2021,
	address = {Nashville, TN, USA},
	title = {{ForgeryNet}: {A} {Versatile} {Benchmark} for {Comprehensive} {Forgery} {Analysis}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{ForgeryNet}},
	url = {https://ieeexplore.ieee.org/document/9577546/},
	doi = {10.1109/CVPR46437.2021.00434},
	abstract = {The rapid progress of photorealistic synthesis techniques have reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Yinan and Gan, Bei and Chen, Siyu and Zhou, Yichun and Yin, Guojun and Song, Luchuan and Sheng, Lu and Shao, Jing and Liu, Ziwei},
	month = jun,
	year = {2021},
	pages = {4358--4367},
	file = {He et al. - 2021 - ForgeryNet A Versatile Benchmark for Comprehensiv.pdf:/Users/lucasjing/Zotero/storage/R3VICK3Q/He et al. - 2021 - ForgeryNet A Versatile Benchmark for Comprehensiv.pdf:application/pdf},
}

@inproceedings{yang_increaco_2021,
	address = {Waikoloa, HI, USA},
	title = {{IncreACO}: {Incrementally} {Learned} {Automatic} {Check}-out with {Photorealistic} {Exemplar} {Augmentation}},
	isbn = {978-1-66540-477-8},
	shorttitle = {{IncreACO}},
	url = {https://ieeexplore.ieee.org/document/9423423/},
	doi = {10.1109/WACV48630.2021.00067},
	abstract = {Automatic check-out (ACO) emerges as an integral component in recent self-service retailing stores, which aims at automatically detecting and counting the randomly placed products upon a check-out platform. Existing data-driven counting works still have difﬁculties in generalizing to realworld retail product counting scenarios, since (1) real check-out images are hard to collect or cover all products and their possible layouts, (2) rapid updating of the product list leads to frequent and tedious re-training of the counting models. To overcome these obstacles, we contribute a practical automatic check-out framework tailored to real-world retail product counting scenarios, consisting of a photorealistic exemplar augmentation to generate physically reliable and photorealistic check-out images from canonical exemplars scanned for each product and an incremental learning strategy to match the updating nature of the ACO system with much fewer training effort. Through comprehensive studies, we show that the proposed IncreACO serves as an effective framework on the recent Retail Product Checkout (RPC) dataset, where the proposed photorealistic exemplar augmentation remarkably improves the counting performance against the state-of-the-art methods (77.15\% v.s. 72.83\% in counting accuracy), whilst the proposed incremental learning framework consistently extends the counting performance to new categories.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2021 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Yang, Yandan and Sheng, Lu and Jiang, Xiaolong and Wang, Haochen and Xu, Dong and Cao, Xianbin},
	month = jan,
	year = {2021},
	pages = {626--634},
	file = {Yang et al. - 2021 - IncreACO Incrementally Learned Automatic Check-ou.pdf:/Users/lucasjing/Zotero/storage/A99Q3Y8V/Yang et al. - 2021 - IncreACO Incrementally Learned Automatic Check-ou.pdf:application/pdf},
}

@article{su_pcg-tal_2021,
	title = {{PCG}-{TAL}: {Progressive} {Cross}-{Granularity} {Cooperation} for {Temporal} {Action} {Localization}},
	volume = {30},
	issn = {1941-0042},
	shorttitle = {{PCG}-{TAL}},
	doi = {10.1109/TIP.2020.3044218},
	abstract = {There are two major lines of works, i.e., anchor-based and frame-based approaches, in the field of temporal action localization. But each line of works is inherently limited to a certain detection granularity and cannot simultaneously achieve high recall rates with accurate action boundaries. In this work, we propose a progressive cross-granularity cooperation (PCG-TAL) framework to effectively take advantage of complementarity between the anchor-based and frame-based paradigms, as well as between two-view clues (i.e., appearance and motion). Specifically, our new Anchor-Frame Cooperation (AFC) module can effectively integrate both two-granularity and two-stream knowledge at the feature and proposal levels, as well as within each AFC module and across adjacent AFC modules. Specifically, the RGB-stream AFC module and the flow-stream AFC module are stacked sequentially to form a progressive localization framework. The whole framework can be learned in an end-to-end fashion, whilst the temporal action localization performance can be gradually boosted in a progressive manner. Our newly proposed framework outperforms the state-of-the-art methods on three benchmark datasets the THUMOS14, ActivityNet v1.3 and UCF-101-24, which clearly demonstrates the effectiveness of our framework.},
	journal = {IEEE Transactions on Image Processing},
	author = {Su, Rui and Xu, Dong and Sheng, Lu and Ouyang, Wanli},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Benchmark testing, cross-granularity cooperation, cross-stream cooperation, Feature extraction, Grammar, Kernel, Proposals, Spatiotemporal phenomena, Temporal action localization, Three-dimensional displays},
	pages = {2103--2113},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/CVMWNC37/9298475.html:text/html},
}

@incollection{vedaldi_thinking_2020,
	address = {Cham},
	title = {Thinking in {Frequency}: {Face} {Forgery} {Detection} by {Mining} {Frequency}-{Aware} {Clues}},
	volume = {12357},
	isbn = {978-3-030-58609-6 978-3-030-58610-2},
	shorttitle = {Thinking in {Frequency}},
	url = {https://link.springer.com/10.1007/978-3-030-58610-2_6},
	abstract = {As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We ﬁnd that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F3-Net), taking advantages of two diﬀerent but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F3-Net signiﬁcantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Qian, Yuyang and Yin, Guojun and Sheng, Lu and Chen, Zixuan and Shao, Jing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58610-2_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {86--103},
	file = {Qian et al. - 2020 - Thinking in Frequency Face Forgery Detection by M.pdf:/Users/lucasjing/Zotero/storage/TH4896JI/Qian et al. - 2020 - Thinking in Frequency Face Forgery Detection by M.pdf:application/pdf},
}

@article{cheung_motion_2021,
	title = {Motion {Compensated} {Virtual} {View} {Synthesis} {Using} {Novel} {Particle} {Cell}},
	volume = {23},
	issn = {1941-0077},
	doi = {10.1109/TMM.2020.3004966},
	abstract = {Due to the wide interest in advanced multimedia experience, free-viewpoint communication is being greatly developed in recent years. In the free-viewpoint communication, viewers can perceive a view from any angle and any position of a scene. Even though the preferred views are not captured, we can generate the views through virtual view synthesis that synthesizes an arbitrary view from captured reference view(s). For daily use, only one or few cameras in baseline distance are given to capture the scene that makes the virtual view synthesis challenging. The task is more difficult when the camera is continuously moving. In this paper, we propose a particle cell to model a reference view sequence to a set of moving particles for virtual view synthesis. Using our novel hybrid motion estimation scheme, the projected coordinates of particles in each frame are obtained even they are occluded. The particles are warped to a virtual view and synthesized as a virtual view sequence. Our method is applicable for both dynamic camera setting and static camera setting. The experimental results show our method outperforms the state-of-the-art algorithms in dynamic camera datasets and presents improvement in static camera datasets in general.},
	journal = {IEEE Transactions on Multimedia},
	author = {Cheung, Chi Ho and Sheng, Lu and Ngan, King Ngi},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Three-dimensional displays, 3D-warping, Cameras, depth-image-based rendering, Free-viewpoint communication, Motion compensation, Motion estimation, Optical imaging, Predictive models, Rendering (computer graphics), virtual view synthesis},
	pages = {1908--1923},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/J7AE9CWQ/9126202.html:text/html},
}

@incollection{vedaldi_powering_2020,
	address = {Cham},
	title = {Powering {One}-{Shot} {Topological} {NAS} with {Stabilized} {Share}-{Parameter} {Proxy}},
	volume = {12359},
	isbn = {978-3-030-58567-9 978-3-030-58568-6},
	url = {https://link.springer.com/10.1007/978-3-030-58568-6_37},
	abstract = {One-shot NAS method has attracted much interest from the research community due to its remarkable training eﬃciency and capacity to discover high performance models. However, the search spaces of previous one-shot based works usually relied on hand-craft design and were short for ﬂexibility on the network topology. In this work, we try to enhance the one-shot NAS by exploring high-performing network architectures in our large-scale Topology Augmented Search Space (i.e, over 3.4 × 1010 diﬀerent topological structures). Speciﬁcally, the diﬃculties for architecture searching in such a complex space has been eliminated by the proposed stabilized share-parameter proxy, which employs Stochastic Gradient Langevin Dynamics to enable fast shared parameter sampling, so as to achieve stabilized measurement of architecture performance even in search space with complex topological structures. The proposed method, namely Stablized Topological Neural Architecture Search (ST-NAS), achieves state-of-the-art performance under Multiply-Adds (MAdds) constraint on ImageNet. Our lite model ST-NAS-A achieves 76.4\% top-1 accuracy with only 326M MAdds. Our moderate model STNAS-B achieves 77.9\% top-1 accuracy just required 503M MAdds. Both of our models oﬀer superior performances in comparison to other concurrent works on one-shot NAS.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Guo, Ronghao and Lin, Chen and Li, Chuming and Tian, Keyu and Sun, Ming and Sheng, Lu and Yan, Junjie},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58568-6_37},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {625--641},
	file = {Guo et al. - 2020 - Powering One-Shot Topological NAS with Stabilized .pdf:/Users/lucasjing/Zotero/storage/SUCJVH38/Guo et al. - 2020 - Powering One-Shot Topological NAS with Stabilized .pdf:application/pdf},
}

@article{sheng_high-quality_2020,
	title = {High-{Quality} {Video} {Generation} from {Static} {Structural} {Annotations}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-020-01334-x},
	doi = {10.1007/s11263-020-01334-x},
	abstract = {This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle-consistent flow-based conditioned variational autoencoder to capture the long-term motion distributions, by which the learned bi-directional flows ensure the physical reliability of the predicted motions and provide explicit occlusion handling in a principled manner. Integrating structural annotations into the flow prediction also improves the structural awareness in the I2V generation process. Quantitative and qualitative evaluations over the autonomous driving and human action datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods. The code has been released: https://github.com/junting/seg2vid.},
	language = {en},
	number = {10},
	urldate = {2022-08-22},
	journal = {International Journal of Computer Vision},
	author = {Sheng, Lu and Pan, Junting and Guo, Jiaming and Shao, Jing and Loy, Chen Change},
	month = nov,
	year = {2020},
	keywords = {Conditioned generative model, Image and video synthesis, Motion prediction and estimatiovn, Unsupervised learning},
	pages = {2552--2569},
}

@inproceedings{liu_morphing_2020,
	title = {Morphing and {Sampling} {Network} for {Dense} {Point} {Cloud} {Completion}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6827},
	doi = {10.1609/aaai.v34i07.6827},
	abstract = {3D point cloud completion, the task of inferring the complete geometric shape from a partial point cloud, has been attracting attention in the community. For acquiring high-fidelity dense point clouds and avoiding uneven distribution, blurred details, or structural loss of existing methods' results, we propose a novel approach to complete the partial point cloud in two stages. Specifically, in the first stage, the approach predicts a complete but coarse-grained point cloud with a collection of parametric surface elements. Then, in the second stage, it merges the coarse-grained prediction with the input point cloud by a novel sampling algorithm. Our method utilizes a joint loss function to guide the distribution of the points. Extensive experiments verify the effectiveness of our method and demonstrate that it outperforms the existing methods in both the Earth Mover's Distance (EMD) and the Chamfer Distance (CD).},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Minghua and Sheng, Lu and Yang, Sheng and Shao, Jing and Hu, Shi-Min},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11596--11603},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/U2TXH7Z8/Liu et al. - 2020 - Morphing and Sampling Network for Dense Point Clou.pdf:application/pdf},
}

@inproceedings{sheng_unsupervised_2019,
	address = {Seoul, Korea (South)},
	title = {Unsupervised {Collaborative} {Learning} of {Keyframe} {Detection} and {Visual} {Odometry} {Towards} {Monocular} {Deep} {SLAM}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010691/},
	doi = {10.1109/ICCV.2019.00440},
	abstract = {In this paper we tackle the joint learning problem of keyframe detection and visual odometry towards monocular visual SLAM systems. As an important task in visual SLAM, keyframe selection helps efﬁcient camera relocalization and effective augmentation of visual odometry. To beneﬁt from it, we ﬁrst present a deep network design for the keyframe selection, which is able to reliably detect keyframes and localize new frames, then an end-to-end unsupervised deep framework further proposed for simultaneously learning the keyframe selection and the visual odometry tasks. As far as we know, it is the ﬁrst work to jointly optimize these two complementary tasks in a single deep framework. To make the two tasks facilitate each other in the learning, a collaborative optimization loss based on both geometric and visual metrics is proposed. Extensive experiments on publicly available datasets (i.e. KITTI raw dataset and its odometry split [12]) clearly demonstrate the effectiveness of the proposed approach, and new state-ofthe-art results are established on the unsupervised depth and pose estimation from monocular video.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Sheng, Lu and Xu, Dan and Ouyang, Wanli and Wang, Xiaogang},
	month = oct,
	year = {2019},
	pages = {4301--4310},
	file = {Sheng et al. - 2019 - Unsupervised Collaborative Learning of Keyframe De.pdf:/Users/lucasjing/Zotero/storage/EHR5QNJT/Sheng et al. - 2019 - Unsupervised Collaborative Learning of Keyframe De.pdf:application/pdf},
}

@inproceedings{tang_improving_2019,
	address = {Seoul, Korea (South)},
	title = {Improving {Pedestrian} {Attribute} {Recognition} {With} {Weakly}-{Supervised} {Multi}-{Scale} {Attribute}-{Specific} {Localization}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010997/},
	doi = {10.1109/ICCV.2019.00510},
	abstract = {Pedestrian attribute recognition has been an emerging research topic in the area of video surveillance. To predict the existence of a particular attribute, it is demanded to localize the regions related to the attribute. However, in this task, the region annotations are not available. How to carve out these attribute-related regions remains challenging. Existing methods applied attribute-agnostic visual attention or heuristic body-part localization mechanisms to enhance the local feature representations, while neglecting to employ attributes to deﬁne local feature areas. We propose a ﬂexible Attribute Localization Module (ALM) to adaptively discover the most discriminative regions and learns the regional features for each attribute at multiple levels. Moreover, a feature pyramid architecture is also introduced to enhance the attribute-speciﬁc localization at low-levels with high-level semantic guidance. The proposed framework does not require additional region annotations and can be trained end-to-end with multi-level deep supervision. Extensive experiments show that the proposed method achieves state-of-the-art results on three pedestrian attribute datasets, including PETA, RAP, and PA-100K.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Tang, Chufeng and Sheng, Lu and Zhang, Zhao-Xiang and Hu, Xiaolin},
	month = oct,
	year = {2019},
	pages = {4996--5005},
	file = {Tang et al. - 2019 - Improving Pedestrian Attribute Recognition With We.pdf:/Users/lucasjing/Zotero/storage/8EXCS9CU/Tang et al. - 2019 - Improving Pedestrian Attribute Recognition With We.pdf:application/pdf},
}

@article{dong_bags_2019,
	title = {Bags of tricks for learning depth and camera motion from monocular videos},
	volume = {1},
	issn = {2096-5796},
	url = {https://www.sciencedirect.com/science/article/pii/S2096579619300658},
	doi = {10.1016/j.vrih.2019.09.004},
	abstract = {Based on the seminal work proposed by Zhou et al., much of the recent progress in learning monocular visual odometry, i. e. depth and camera motion fr…},
	language = {en},
	number = {5},
	urldate = {2022-08-22},
	journal = {Virtual Reality \& Intelligent Hardware},
	author = {Dong, Bowen and Sheng, Lu},
	month = oct,
	year = {2019},
	note = {Publisher: Elsevier},
	pages = {500--510},
	file = {Full Text:/Users/lucasjing/Zotero/storage/RTGABS5Y/2019 - Bags of tricks for learning depth and camera motio.pdf:application/pdf;Snapshot:/Users/lucasjing/Zotero/storage/58DLTNPM/S2096579619300658.html:text/html},
}

@inproceedings{wang_camp_2019,
	address = {Seoul, Korea (South)},
	title = {{CAMP}: {Cross}-{Modal} {Adaptive} {Message} {Passing} for {Text}-{Image} {Retrieval}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{CAMP}},
	url = {https://ieeexplore.ieee.org/document/9010827/},
	doi = {10.1109/ICCV.2019.00586},
	abstract = {Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wang, Zihao and Liu, Xihui and Li, Hongsheng and Sheng, Lu and Yan, Junjie and Wang, Xiaogang and Shao, Jing},
	month = oct,
	year = {2019},
	pages = {5763--5772},
	file = {Wang et al. - 2019 - CAMP Cross-Modal Adaptive Message Passing for Tex.pdf:/Users/lucasjing/Zotero/storage/DYM4P696/Wang et al. - 2019 - CAMP Cross-Modal Adaptive Message Passing for Tex.pdf:application/pdf},
}

@article{wu_cascaded_2019,
	title = {Cascaded regression using landmark displacement for {3D} face reconstruction},
	volume = {125},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865518303933},
	doi = {10.1016/j.patrec.2019.07.017},
	abstract = {This paper proposes a novel model fitting algorithm for 3D facial expression reconstruction from a single image. Face expression reconstruction from a single image is a challenging task in computer vision. Most state-of-the-art methods fit the input image to a 3D Morphable Model (3DMM). These methods need to solve a stochastic problem and cannot deal with expression and pose variations. To solve this problem, we adopt a 3D face expression model and use a combined feature which is robust to scale, rotation and different lighting conditions. The proposed method applies a cascaded regression framework to estimate parameters for the 3DMM. 2D landmarks are detected and used to initialize the 3D shape and mapping matrices. In each iteration, residues between the current 3DMM parameters and the ground truth are estimated and then used to update the 3D shapes. The mapping matrices are also calculated based on the updated shapes and 2D landmarks. HOG features of the local patches and displacements between 3D landmark projections and 2D landmarks are exploited. Compared with existing methods, the proposed method is robust to expression and pose changes and can reconstruct higher fidelity 3D face shape.},
	language = {en},
	urldate = {2022-08-22},
	journal = {Pattern Recognition Letters},
	author = {Wu, Fanzi and Li, Songnan and Zhao, Tianhao and Ngan, King Ngi and Sheng, Lu},
	month = jul,
	year = {2019},
	keywords = {3DMM, Cascaded regression, Facial expression, Landmarks},
	pages = {766--772},
	file = {ScienceDirect Snapshot:/Users/lucasjing/Zotero/storage/GTSX6ZMZ/S0167865518303933.html:text/html},
}

@inproceedings{pan_video_2019,
	address = {Long Beach, CA, USA},
	title = {Video {Generation} {From} {Single} {Semantic} {Label} {Map}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953551/},
	doi = {10.1109/CVPR.2019.00385},
	abstract = {This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between ﬂexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difﬁcult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the ﬁrst frame. Then we animate the scene based on its semantic meaning to obtain temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical ﬂow as a beneﬁcial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the ﬂow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods. The source code will be released on https://github.com/junting/seg2vid.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pan, Junting and Wang, Chengyu and Jia, Xu and Shao, Jing and Sheng, Lu and Yan, Junjie and Wang, Xiaogang},
	month = jun,
	year = {2019},
	pages = {3728--3737},
	file = {Pan et al. - 2019 - Video Generation From Single Semantic Label Map.pdf:/Users/lucasjing/Zotero/storage/FDP7NVUN/Pan et al. - 2019 - Video Generation From Single Semantic Label Map.pdf:application/pdf},
}

@inproceedings{li_gs3d_2019,
	address = {Long Beach, CA, USA},
	title = {{GS3D}: {An} {Efficient} {3D} {Object} {Detection} {Framework} for {Autonomous} {Driving}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{GS3D}},
	url = {https://ieeexplore.ieee.org/document/8954005/},
	doi = {10.1109/CVPR.2019.00111},
	abstract = {We present an efﬁcient 3D object detection framework based on a single RGB image in the scenario of autonomous driving. Our efforts are put on extracting the underlying 3D information in a 2D image and determining the accurate 3D bounding box of the object without point cloud or stereo data. Leveraging the off-the-shelf 2D object detector, we propose an artful approach to efﬁciently obtain a coarse cuboid for each predicted 2D box. The coarse cuboid has enough accuracy to guide us to determine the 3D box of the object by reﬁnement. In contrast to previous state-ofthe-art methods that only use the features extracted from the 2D bounding box for box reﬁnement, we explore the 3D structure information of the object by employing the visual features of visible surfaces. The new features from surfaces are utilized to eliminate the problem of representation ambiguity brought by only using a 2D bounding box. Moreover, we investigate different methods of 3D box reﬁnement and discover that a classiﬁcation formulation with quality aware loss has much better performance than regression. Evaluated on the KITTI benchmark, our approach outperforms current state-of-the-art methods for single RGB image based 3D object detection.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Buyu and Ouyang, Wanli and Sheng, Lu and Zeng, Xingyu and Wang, Xiaogang},
	month = jun,
	year = {2019},
	pages = {1019--1028},
	file = {Li et al. - 2019 - GS3D An Efficient 3D Object Detection Framework f.pdf:/Users/lucasjing/Zotero/storage/N5VIIQJY/Li et al. - 2019 - GS3D An Efficient 3D Object Detection Framework f.pdf:application/pdf},
}

@article{sheng_visibility_2019,
	title = {Visibility {Constrained} {Generative} {Model} for {Depth}-{Based} {3D} {Facial} {Pose} {Tracking}},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2877675},
	abstract = {In this paper, we propose a generative framework that unifies depth-based 3D facial pose tracking and face model adaptation on-the-fly, in the unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Specifically, we introduce a statistical 3D morphable model that flexibly describes the distribution of points on the surface of the face model, with an efficient switchable online adaptation that gradually captures the identity of the tracked subject and rapidly constructs a suitable face model when the subject changes. Moreover, unlike prior art that employed ICP-based facial pose estimation, to improve robustness to occlusions, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility with respect to the input point cloud. Ablation studies and experimental results on Biwi and ICT-3DHP datasets demonstrate that the proposed framework is effective and outperforms completing state-of-the-art depth-based methods.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Sheng, Lu and Cai, Jianfei and Cham, Tat-Jen and Pavlovic, Vladimir and Ngan, King Ngi},
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Three-dimensional displays, 3D facial pose tracking, Adaptation models, depth, Face, generative model, mixture of Gaussian models, online Bayesian model, Pose estimation, Shape, Solid modeling, Switches},
	pages = {1994--2007},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/MCUUSL5I/8502935.html:text/html;Submitted Version:/Users/lucasjing/Zotero/storage/8H9Q5A2V/Sheng et al. - 2019 - Visibility Constrained Generative Model for Depth-.pdf:application/pdf},
}

@inproceedings{yin_context_2019,
	address = {Long Beach, CA, USA},
	title = {Context and {Attribute} {Grounded} {Dense} {Captioning}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954079/},
	doi = {10.1109/CVPR.2019.00640},
	abstract = {Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to the state-of-the-art methods.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yin, Guojun and Sheng, Lu and Liu, Bin and Yu, Nenghai and Wang, Xiaogang and Shao, Jing},
	month = jun,
	year = {2019},
	pages = {6234--6243},
	file = {Yin et al. - 2019 - Context and Attribute Grounded Dense Captioning.pdf:/Users/lucasjing/Zotero/storage/GKMA6G32/Yin et al. - 2019 - Context and Attribute Grounded Dense Captioning.pdf:application/pdf},
}

@inproceedings{liu_multi-label_2018,
	address = {New York, NY, USA},
	series = {{MM} '18},
	title = {Multi-{Label} {Image} {Classification} via {Knowledge} {Distillation} from {Weakly}-{Supervised} {Detection}},
	isbn = {978-1-4503-5665-7},
	url = {https://doi.org/10.1145/3240508.3240567},
	doi = {10.1145/3240508.3240567},
	abstract = {Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.},
	urldate = {2022-08-21},
	booktitle = {Proceedings of the 26th {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yongcheng and Sheng, Lu and Shao, Jing and Yan, Junjie and Xiang, Shiming and Pan, Chunhong},
	year = {2018},
	keywords = {knowledge distillation, multi-label image classification, weakly-supervised detection},
	pages = {700--708},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/EFCHPNQ7/Liu et al. - 2018 - Multi-Label Image Classification via Knowledge Dis.pdf:application/pdf},
}

@incollection{ferrari_zoom-net_2018,
	address = {Cham},
	title = {Zoom-{Net}: {Mining} {Deep} {Feature} {Interactions} for {Visual} {Relationship} {Recognition}},
	volume = {11207},
	isbn = {978-3-030-01218-2 978-3-030-01219-9},
	shorttitle = {Zoom-{Net}},
	url = {http://link.springer.com/10.1007/978-3-030-01219-9_20},
	abstract = {Recognizing visual relationships subject-predicate-object among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features. The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our ﬁnal Zoom-Net. We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the eﬀectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 11.42\% from 8.16\%) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Yin, Guojun and Sheng, Lu and Liu, Bin and Yu, Nenghai and Wang, Xiaogang and Shao, Jing and Loy, Chen Change},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01219-9_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {330--347},
	file = {Yin et al. - 2018 - Zoom-Net Mining Deep Feature Interactions for Vis.pdf:/Users/lucasjing/Zotero/storage/5B8D377A/Yin et al. - 2018 - Zoom-Net Mining Deep Feature Interactions for Vis.pdf:application/pdf},
}

@inproceedings{sun_optical_2018,
	address = {Salt Lake City, UT},
	title = {Optical {Flow} {Guided} {Feature}: {A} {Fast} and {Robust} {Motion} {Representation} for {Video} {Action} {Recognition}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Optical {Flow} {Guided} {Feature}},
	url = {https://ieeexplore.ieee.org/document/8578249/},
	doi = {10.1109/CVPR.2018.00151},
	abstract = {Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatio-temporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatiotemporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3\% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96.0\% and 74.2\% accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at: https://github.com/kevin-ssy/Optical-Flow-Guided-Feature},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Sun, Shuyang and Kuang, Zhanghui and Sheng, Lu and Ouyang, Wanli and Zhang, Wei},
	month = jun,
	year = {2018},
	pages = {1390--1399},
	file = {Sun et al. - 2018 - Optical Flow Guided Feature A Fast and Robust Mot.pdf:/Users/lucasjing/Zotero/storage/GJB8UXNH/Sun et al. - 2018 - Optical Flow Guided Feature A Fast and Robust Mot.pdf:application/pdf},
}

@article{cheung_spatio-temporal_2018,
	title = {Spatio-{Temporal} {Disocclusion} {Filling} {Using} {Novel} {Sprite} {Cells}},
	volume = {20},
	issn = {1941-0077},
	doi = {10.1109/TMM.2017.2772442},
	abstract = {Depth image-based rendering is an important technique for virtual view synthesis with limited 3-D data. However, occluded areas result in disocclusions in synthesized images. Filling of the disocclusions in a plausible manner is a critical task in virtual view synthesis. In addition to spatial consistency, temporal consistency of the filled regions also affects the visual quality. In this paper, we propose a novel codebook method called sprite cell for filling the disocclusions with high spatial and temporal consistency. Each codeword consists of a color vector, depth value, frame log, and the confidence score of the corresponding pixel. In contrast with the existing methods that reuse the filling results of previous frames without considering their accuracy, the proposed method estimates the confidence scores of the filling results to prevent temporal continuation of filling errors. Moreover, we introduce a method to correct the luminance of filled disocclusions that compensates for the change of scenes. The experimental results show that the proposed method achieves both objective and subjective improvements over the state-of-the-art methods. The sequences synthesized with the proposed method have higher spatio-temporal consistency.},
	number = {6},
	journal = {IEEE Transactions on Multimedia},
	author = {Cheung, Chi Ho and Ngan, King Ngi and Sheng, Lu},
	month = jun,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Cameras, Rendering (computer graphics), 3-D, depth image-based rendering, Disocclusion filling, Distortion, Filling, Image color analysis, inpainting, Sprites (computer)},
	pages = {1376--1391},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/YLJYR2G9/8103880.html:text/html},
}

@inproceedings{liu_exploring_2018,
	address = {Salt Lake City, UT, USA},
	title = {Exploring {Disentangled} {Feature} {Representation} {Beyond} {Face} {Identification}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578320/},
	doi = {10.1109/CVPR.2018.00222},
	abstract = {This paper proposes learning disentangled but complementary face features with a minimal supervision by face identiﬁcation. Speciﬁcally, we construct an identity Distilling and Dispelling Autoencoder (D2AE) framework that adversarially learns the identity-distilled features for identity veriﬁcation and the identity-dispelled features to fool the veriﬁcation system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only preserve state-of-the-art identity veriﬁcation performance on LFW, but also acquire comparable discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Liu, Yu and Wei, Fangyin and Shao, Jing and Sheng, Lu and Yan, Junjie and Wang, Xiaogang},
	month = jun,
	year = {2018},
	pages = {2080--2089},
	file = {Liu et al. - 2018 - Exploring Disentangled Feature Representation Beyo.pdf:/Users/lucasjing/Zotero/storage/6U3R3L6I/Liu et al. - 2018 - Exploring Disentangled Feature Representation Beyo.pdf:application/pdf},
}

@inproceedings{sheng_avatar-net_2018,
	address = {Salt Lake City, UT, USA},
	title = {Avatar-{Net}: {Multi}-scale {Zero}-{Shot} {Style} {Transfer} by {Feature} {Decoration}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Avatar-{Net}},
	url = {https://ieeexplore.ieee.org/document/8578958/},
	doi = {10.1109/CVPR.2018.00860},
	abstract = {Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efﬁciency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efﬁcient yet effective Avatar-Net that enables visually plausible multiscale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multiscale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efﬁciency of the proposed method in generating high-quality stylized images, with a series of successive applications include multiple style integration, video stylization and etc.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Sheng, Lu and Lin, Ziyi and Shao, Jing and Wang, Xiaogang},
	month = jun,
	year = {2018},
	pages = {8242--8250},
	file = {Sheng et al. - 2018 - Avatar-Net Multi-scale Zero-Shot Style Transfer b.pdf:/Users/lucasjing/Zotero/storage/D4NMCTNG/Sheng et al. - 2018 - Avatar-Net Multi-scale Zero-Shot Style Transfer b.pdf:application/pdf},
}

@inproceedings{yin_semantics_2019,
	title = {Semantics {Disentangling} for {Text}-{To}-{Image} {Generation}},
	doi = {10.1109/CVPR.2019.00243},
	abstract = {Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Yin, Guojun and Liu, Bin and Sheng, Lu and Yu, Nenghai and Wang, Xiaogang and Shao, Jing},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Image and Video Synthesis, Vision + Language},
	pages = {2322--2331},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/F7SEVDTT/8953563.html:text/html;Yin et al. - Semantics Disentangling for Text-To-Image Generati.pdf:/Users/lucasjing/Zotero/storage/85LFTAGI/Yin et al. - Semantics Disentangling for Text-To-Image Generati.pdf:application/pdf},
}

@inproceedings{liu_hydraplus-net_2017,
	address = {Venice},
	title = {{HydraPlus}-{Net}: {Attentive} {Deep} {Features} for {Pedestrian} {Analysis}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {{HydraPlus}-{Net}},
	url = {http://ieeexplore.ieee.org/document/8237308/},
	doi = {10.1109/ICCV.2017.46},
	abstract = {Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attentionbased deep neural network, named as HydraPlus-Net (HPnet), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person reidentification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-theart methods on various datasets.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Liu, Xihui and Zhao, Haiyu and Tian, Maoqing and Sheng, Lu and Shao, Jing and Yi, Shuai and Yan, Junjie and Wang, Xiaogang},
	month = oct,
	year = {2017},
	pages = {350--359},
	file = {Liu et al. - 2017 - HydraPlus-Net Attentive Deep Features for Pedestr.pdf:/Users/lucasjing/Zotero/storage/XNKBX5AP/Liu et al. - 2017 - HydraPlus-Net Attentive Deep Features for Pedestr.pdf:application/pdf},
}

@inproceedings{sheng_generative_2017,
	address = {Honolulu, HI},
	title = {A {Generative} {Model} for {Depth}-{Based} {Robust} {3D} {Facial} {Pose} {Tracking}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099972/},
	doi = {10.1109/CVPR.2017.489},
	abstract = {We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that uniﬁes pose tracking and face model adaptation on-the-ﬂy. Particularly, we propose a statistical 3D face model that owns the ﬂexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face model’s visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Sheng, Lu and Cai, Jianfei and Cham, Tat-Jen and Pavlovic, Vladimir and Ngan, King Ngi},
	month = jul,
	year = {2017},
	pages = {4598--4607},
	file = {Sheng et al. - 2017 - A Generative Model for Depth-Based Robust 3D Facia.pdf:/Users/lucasjing/Zotero/storage/QCJS64EV/Sheng et al. - 2017 - A Generative Model for Depth-Based Robust 3D Facia.pdf:application/pdf},
}

@article{li_real-time_2016,
	title = {Real-{Time} {Head} {Pose} {Tracking} with {Online} {Face} {Template} {Reconstruction}},
	volume = {38},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2015.2500221},
	abstract = {We propose a real-time method to accurately track the human head pose in the 3-dimensional (3D) world. Using a RGB-Depth camera, a face template is reconstructed by fitting a 3D morphable face model, and the head pose is determined by registering this user-specific face template to the input depth video.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Li, Songnan and Ngan, King Ngi and Paramesran, Raveendran and Sheng, Lu},
	month = sep,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Three-dimensional displays, Cameras, Face, deformable face model, Head pose tracking, Image reconstruction, iterative closest point, Magnetic heads, RGB-Depth camera, Tracking},
	pages = {1922--1928},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/KSLFGPHI/7328312.html:text/html},
}

@article{sheng_online_2015,
	title = {Online {Temporally} {Consistent} {Indoor} {Depth} {Video} {Enhancement} via {Static} {Structure}},
	volume = {24},
	issn = {1941-0042},
	doi = {10.1109/TIP.2015.2416658},
	abstract = {In this paper, we propose a new method to online enhance the quality of a depth video based on the intermediary of a so-called static structure of the captured scene. The static and dynamic regions of the input depth frame are robustly separated by a layer assignment procedure, in which the dynamic part stays in the front while the static part fits and helps to update this structure by a novel online variational generative model with added spatial refinement. The dynamic content is enhanced spatially while the static region is otherwise substituted by the updated static structure so as to favor the long-range spatio-temporal enhancement. The proposed method both performs long-range temporal consistency on the static region and keeps necessary depth variations in the dynamic content. Thus, it can produce flicker-free and spatially optimized depth videos with reduced motion blur and depth distortion. Our experimental results reveal that the proposed method is effective in both static and dynamic indoor scenes and is compatible with depth videos captured by Kinect and time-of-flight camera. We also demonstrate that excellent performance can be achieved by the proposed method in comparison with the existing spatio-temporal approaches. In addition, our enhanced depth videos and static structures can act as effective cues to improve various applications, including depth-aided background subtraction and novel view synthesis, showing satisfactory results with few visual artifacts.},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Sheng, Lu and Ngan, King Ngi and Lim, Chern-Loon and Li, Songnan},
	month = jul,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Three-dimensional displays, Data models, Dynamics, Image processing, layer assignment, Noise, online estimation, Robustness, Static structure, Streaming media, temporally consistent depth, temporally consistent depth video enhancement, video enhancement},
	pages = {2197--2211},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/KDHYWNEX/7067353.html:text/html},
}

@inproceedings{sheng_accelerating_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Accelerating the {Distribution} {Estimation} for the {Weighted} {Median}/{Mode} {Filters}},
	isbn = {978-3-319-16817-3},
	doi = {10.1007/978-3-319-16817-3_1},
	abstract = {Various image filters for applications in the area of computer vision require the properties of the local statistics of the input image, which are always defined by the local distribution or histogram. But the huge expense of computing the distribution hampers the popularity of these filters in real-time or interactive-rate systems. In this paper, we present an efficient and practical method to estimate the local weighted distribution for the weighted median/mode filters based on the kernel density estimation with a new separable kernel defined by a weighted combinations of a series of probabilistic generative models. It reduces the large number of filtering operations in previous constant time algorithms [1, 2] to a small amount, which is also adaptive to the structure of the input image. The proposed accelerated weighted median/mode filters are effective and efficient for a variety of applications, which have comparable performance against the current state-of-the-art counterparts and cost only a fraction of their execution time.},
	language = {en},
	booktitle = {Computer {Vision} -- {ACCV} 2014},
	publisher = {Springer International Publishing},
	author = {Sheng, Lu and Ngan, King Ngi and Hui, Tak-Wai},
	editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
	year = {2015},
	keywords = {Bilateral Filter, Input Image, Kernel Density Estimation, Local Distribution, Local Histogram},
	pages = {3--17},
}

@inproceedings{sheng_temporal_2014,
	title = {Temporal depth video enhancement based on intrinsic static structure},
	doi = {10.1109/ICIP.2014.7025585},
	abstract = {Depth video enhancement is an essential preprocessing step for various 3D applications. Despite extensive studies of spatial enhancement, effective temporal enhancement that both strengthens temporal consistency and keeps correct depth variation needs further research. In this paper, we propose a novel method to enhance the depth video by blending raw depth frame with the estimated intrinsic static structure, which defines static structure of captured scene and is estimated iteratively by a probabilistic generative model with sequentially incoming depth frames. Our experimental results show that the proposed method is effective both in static and dynamic scene and is compatible with various kinds of depth videos. We will demonstrate that superior performance can be achieved in comparison with existing temporal enhancement approaches.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Sheng, Lu and Ngan, King Ngi and Li, Songnan},
	year = {2014},
	note = {ISSN: 2381-8549},
	keywords = {Cameras, Approximation methods, depth video enhancement, Estimation, Joints, Parameter estimation, Probabilistic logic, probabilistic model, Reliability, temporal enhancement, variational approximation},
	pages = {2893--2897},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/56Q2DENT/7025585.html:text/html},
}

@inproceedings{li_screen-camera_2014,
	title = {Screen-camera calibration using a thread},
	doi = {10.1109/ICIP.2014.7025698},
	abstract = {In this paper, we propose a novel screen-camera calibration algorithm which aims to locate the position of the screen in the camera coordinate system. The difficulty comes from the fact that the screen is not directly visible to the camera. Rather than using an external camera or a portable mirror like in previous studies, we propose to use a more accessible and cheaper calibrating object, i.e., a thread. The thread is manipulated so that our algorithm can infer the perspective projections of the four screen corners on the image plane. The 3-dimentional (3D) position of each screen corner is then determined by minimizing the sum of squared projection errors. Experiments show that compared with the previous studies our method can generate similar calibration results without the additional hardware.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Li, Songnan and Ngan, King Ngi and Sheng, Lu},
	year = {2014},
	note = {ISSN: 2381-8549},
	keywords = {Three-dimensional displays, Cameras, Image color analysis, Calibration, Hardware, Hough line detection, Mirrors, Nickel, RANSAC algorithm, Screen-camera calibration},
	pages = {3435--3439},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/FFYZ8R42/7025698.html:text/html},
}

@inproceedings{sheng_depth_2013,
	title = {Depth enhancement based on hybrid geometric hole filling strategy},
	doi = {10.1109/ICIP.2013.6738448},
	abstract = {Depth map is a crucial component in various 3D applications. However, current available depth maps usually suffer from low resolution, high noise, random and structural depth missing problems due to theoretical, systematic or hardware limitations. In this paper, we propose a novel method to enhance depth map with the guidance of aligned color image, tackling these problems in a whole framework, where a hybrid strategy on filling hole geometrically by the combination of joint bilateral filtering and segment-based surface structure propagation is introduced. Our experimental results prove the proposed method outperforms existing methods.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Image} {Processing}},
	author = {Sheng, Lu and Ngan, King Ngi},
	month = sep,
	year = {2013},
	note = {ISSN: 2381-8549},
	keywords = {depth enhancement, Depth map, filtering, hole filling, segmentation},
	pages = {2173--2176},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/SVLZD3AP/6738448.html:text/html},
}

@inproceedings{li_head_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Head} {Pose} {Tracking} {System} {Using} {RGB}-{D} {Camera}},
	isbn = {978-3-642-39402-7},
	doi = {10.1007/978-3-642-39402-7_16},
	abstract = {In this paper, a fast head pose tracking system is introduced. It uses iterative closest point algorithm to register a dense face template to depth data captured by Kinect. It can achieve 33fps processing speed without specific optimization. To improve tracking robustness, head movement prediction is applied. We propose a novel scheme that can train several simple predictors together, enhancing the overall prediction accuracy. Experimental results confirm its effectiveness for head movement prediction.},
	language = {en},
	booktitle = {Computer {Vision} {Systems}},
	publisher = {Springer},
	author = {Li, Songnan and Ngan, King Ngi and Sheng, Lu},
	editor = {Chen, Mei and Leibe, Bastian and Neumann, Bernd},
	year = {2013},
	keywords = {head movement prediction, head pose tracking, iterative closest point algorithm, K-means like retraining},
	pages = {153--162},
}

@inproceedings{cheung_disocclusion_2015,
	title = {A disocclusion filling method using multiple sprites with depth for virtual view synthesis},
	doi = {10.1109/ICMEW.2015.7169773},
	abstract = {Depth image based rendering (DIBR) is an important technique to generate virtual view images with limited 3-D data. However, disocclusion is a critical problem that the regions occluded by foreground objects become visible in virtual views, which is difficult to be visual-plausibly inferred. In this paper, we propose a novel temporally consistent filling method using multiple sprites with depth (MSD) to fill the disocclusions faithfully. MSD stores the background as well as intermediate foreground objects of past frames in multiple sprites, so that amounts of pixels in disocclusions can be recovered from MSD. Moreover, we also introduce a method to recognize the source space for exemplar based inpainting to fill the remaining disocclusions. The experimental results show the proposed method achieves objective and subjective improvement compared with the state-of-the-art methods. The synthesized sequences by the proposed method have higher spatio-temporal consistency.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Multimedia} \& {Expo} {Workshops} ({ICMEW})},
	author = {Cheung, Chi Ho and Sheng, Lu and Ngan, King Ngi},
	month = jun,
	year = {2015},
	keywords = {Three-dimensional displays, Rendering (computer graphics), 3-D, Disocclusion filling, Filling, Image color analysis, inpainting, Sprites (computer), Robustness, depth image based rendering, multiple sprites with depth},
	pages = {1--6},
}

@inproceedings{cai_3djcg_2022,
	title = {{3DJCG}: {A} {Unified} {Framework} for {Joint} {Dense} {Captioning} and {Visual} {Grounding} on {3D} {Point} {Clouds}},
	shorttitle = {{3DJCG}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Cai_3DJCG_A_Unified_Framework_for_Joint_Dense_Captioning_and_Visual_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cai, Daigang and Zhao, Lichen and Zhang, Jing and Sheng, Lu and Xu, Dong},
	year = {2022},
	pages = {16464--16473},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/QRHWDDUI/Cai et al. - 2022 - 3DJCG A Unified Framework for Joint Dense Caption.pdf:application/pdf},
}

@inproceedings{li_danceformer_2022,
	title = {{DanceFormer}: {Music} {Conditioned} {3D} {Dance} {Generation} with {Parametric} {Motion} {Transformer}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{DanceFormer}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20014},
	doi = {10.1609/aaai.v36i2.20014},
	abstract = {Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics. Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements. In this paper, we reformulate it by a two-stage process, i.e., a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements. We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively. Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture. This dataset also encodes dances as key poses and parametric motion curves apart from pose sequences, thus benefiting the training of our DanceFormer. Extensive experiments demonstrate that the proposed method, even trained by existing datasets, can generate fluent, performative, and music-matched 3D dances that surpass previous works quantitatively and qualitatively. Moreover, the proposed DanceFormer, together with the PhantomDance dataset, are seamlessly compatible with industrial animation software, thus facilitating the adaptation for various downstream applications.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Buyu and Zhao, Yongchi and Shi, Zhelun and Sheng, Lu},
	month = jun,
	year = {2022},
	note = {Number: 2},
	keywords = {Domain(s) Of Application (APP)},
	pages = {1272--1279},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/4FAPT7TC/Li et al. - 2022 - DanceFormer Music Conditioned 3D Dance Generation.pdf:application/pdf},
}

@article{wang_vpu_2022,
	title = {{VPU}: {A} {Video}-{Based} {Point} {Cloud} {Upsampling} {Framework}},
	volume = {31},
	issn = {1941-0042},
	shorttitle = {{VPU}},
	doi = {10.1109/TIP.2022.3166627},
	abstract = {In this work, we propose a new patch-based framework called VPU for the video-based point cloud upsampling task by effectively exploiting temporal dependency among multiple consecutive point cloud frames, in which each frame consists of a set of unordered, sparse and irregular 3D points. Rather than adopting the sophisticated motion estimation strategy in video analysis, we propose a new spatio-temporal aggregation (STA) module to effectively extract, align and aggregate rich local geometric clues from consecutive frames at the feature level. By more reliably summarizing spatio-temporally consistent and complementary knowledge from multiple frames in the resultant local structural features, our method better infers the local geometry distributions at the current frame. In addition, our STA module can be readily incorporated with various existing single frame-based point upsampling methods (e.g., PU-Net, MPU, PU-GAN and PU-GCN). Comprehensive experiments on multiple point cloud sequence datasets demonstrate our video-based point cloud upsampling framework achieves substantial performance improvement over its single frame-based counterparts.},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Kaisiyuan and Sheng, Lu and Gu, Shuhang and Xu, Dong},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Cloud computing, Feature extraction, Graphics processing units, Image reconstruction, Point cloud compression, Point cloud sequence, point cloud upsampling, spatial-temporal aggregation, Task analysis, Three-dimensional displays},
	pages = {4062--4075},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/GA9Q9MUD/9759233.html:text/html},
}
